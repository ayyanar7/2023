{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMKMG3lNIIvZJcr3irSR9bC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayyanar7/2023/blob/main/Text2Imagegen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dRFaarbGM465",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1ee824c-8fda-42f9-cd11-7a2f0210a905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/flowershd5dataset\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "# import kagglehub\n",
        "# ds_path = kagglehub.dataset_download('kmahesh541/flowershd5dataset')\n",
        "# words_path = kagglehub.dataset_download('msripooja/flowershd5words')\n",
        "# mast_d_path = kagglehub.dataset_download('kaushalmak07/mast-d')\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"kmahesh541/flowershd5dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print('Dataset path.',ds_path)\n",
        "# print('words path.',words_path)\n",
        "# print('mast path.',mast_d_path)\n",
        "hdf5_fpath = path+\"/flowers-hd5/data/flowers/flowers.hdf5\"\n",
        "print(hdf5_fpath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g64mkDOKrbdw",
        "outputId": "6de9a686-ec6b-4c33-f2ec-f446a608dfd0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/kaggle/input/flowershd5dataset/flowers-hd5/data/flowers/flowers.hdf5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import io\n",
        "import h5py\n",
        "import argparse\n",
        "import torch\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torch import nn\n",
        "from datetime import timedelta\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "print(\"All libraries imported!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44UTxyM_skvJ",
        "outputId": "758d9069-6e14-4428-b455-bbd7b7ed3b8c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = h5py.File(hdf5_fpath)\n",
        "\n",
        "#1. to know the categories in hdf5 file\n",
        "print(list(f))\n",
        "print(\"\\nNo. of items in test = \",len(list(f['test'])))\n",
        "print(\"\\nNo. of items in train = \",len(list(f['train'])))\n",
        "print(\"\\nNo. of items in valid = \",len(list(f['valid'])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPcZ-3jLtGNd",
        "outputId": "5d55fffa-0af5-4252-ce41-848a849f127d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['test', 'train', 'valid']\n",
            "\n",
            "No. of items in test =  5775\n",
            "\n",
            "No. of items in train =  29390\n",
            "\n",
            "No. of items in valid =  5780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NearestNeighbor:\n",
        "    def __init__(self, dataset, source, cuda, ngf):\n",
        "        self.dataset = dataset\n",
        "        data = None\n",
        "        representation = None\n",
        "        labels = []\n",
        "        embeddings = []\n",
        "        path = ''\n",
        "        data_path = path + 'source_{}_nn_data.pl'.format(source)\n",
        "        labels_path = path + 'source_{}_nn_labels.pl'.format(source)\n",
        "        nbrs_path = path + 'source_{}_nn.pl'.format(source)\n",
        "        embeddings_path = path + 'source_{}_nn_embeddings.pl'.format(source)\n",
        "        print(\"data_path: \",data_path)\n",
        "        print(\"data_path: \"+labels_path)\n",
        "        print(\"data_path: \"+nbrs_path)\n",
        "        print(\"data_path: \"+embeddings_path)\n",
        "        self.model = gan_factory.generator_factory('vae', ngf, False)\n",
        "        if cuda:\n",
        "            self.model = self.model.cuda()\n",
        "        #self.model.load_state_dict(torch.load('./checkpoints/flowers_autoencoder/gen.pth'))\n",
        "\n",
        "        if os.path.exists(data_path):\n",
        "            print('start loading data for NN test {}'.format(data_path))\n",
        "            data = pickle.load(open(data_path, 'rb'))\n",
        "            labels = pickle.load(open(labels_path, 'rb'))\n",
        "            nbrs = pickle.load(open(nbrs_path, 'rb'))\n",
        "            embeddings = pickle.load(open(embeddings_path, 'rb'))\n",
        "        else:\n",
        "            print('start creating data for NN test {}'.format(data_path))\n",
        "            for i, sample in enumerate(dataset):\n",
        "                #print(\"**** iter i = \",i)\n",
        "                if data is None:\n",
        "                    data = sample['right_images'].numpy()\n",
        "                    data_var = Variable(sample['right_images'].float(), volatile=True)\n",
        "                    if cuda:\n",
        "                        data_var = data_var.cuda()\n",
        "                    representation = self.model.encoder_only(data_var).data.cpu().numpy()\n",
        "                    labels = sample['txt']\n",
        "                    embeddings = sample['right_embed']\n",
        "                else:\n",
        "                    data = np.append(data, sample['right_images'].numpy(), axis=0)\n",
        "                    data_var = Variable(sample['right_images'].float(), volatile=True)\n",
        "                    if cuda:\n",
        "                        data_var = data_var.cuda()\n",
        "                    representation = np.append(representation, self.model.encoder_only(data_var).data.cpu().numpy(),\n",
        "                                               axis=0)\n",
        "                    labels += sample['txt']\n",
        "                    embeddings = np.append(embeddings, sample['right_embed'].numpy(), axis=0)\n",
        "            nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(representation.reshape(-1, 228))\n",
        "            pickle.dump(data, open(data_path, 'wb'))\n",
        "            pickle.dump(labels, open(labels_path, 'wb'))\n",
        "            pickle.dump(nbrs, open(nbrs_path, 'wb'))\n",
        "            pickle.dump(embeddings, open(embeddings_path, 'wb'))\n",
        "        print('finish loading data for NN test')\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.nbrs = nbrs\n",
        "        self.embeddings = embeddings\n",
        "\n",
        "    def get_text(self, samples, limit=-1):\n",
        "        text_results, _, _ = self.get_text_and_images_and_embedding(samples, limit)\n",
        "        return text_results\n",
        "\n",
        "    def get_text_and_images(self, samples, limit):\n",
        "        text_results, image_results, _ = self.get_text_and_images_and_embedding(samples, limit)\n",
        "        return text_results, image_results\n",
        "\n",
        "    def get_text_and_images_and_embedding(self, samples, limit=-1):\n",
        "        samples_embedding = self.model.encoder_only(samples).data.cpu().numpy().reshape(-1, 228)\n",
        "        if limit != -1:\n",
        "            samples_embedding = samples_embedding[:limit]\n",
        "        distances, indices = self.nbrs.kneighbors(samples_embedding)\n",
        "        text_results = [self.labels[index] for index in indices[:, 0]]\n",
        "        image_results = [self.data[index] for index in indices[:, 0]]\n",
        "        embedding_results = [self.embeddings[index] for index in indices[:, 0]]\n",
        "        return text_results, image_results, embedding_results"
      ],
      "metadata": {
        "id": "JZBQG45wtZMM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################ txt2image_dataset.py ###################\n",
        "\n",
        "class Text2ImageDataset(Dataset):\n",
        "\n",
        "    def __init__(self, datasetFile, transform=None, split=0):\n",
        "        self.datasetFile = datasetFile\n",
        "        self.transform = transform\n",
        "        self.dataset = None\n",
        "        self.dataset_keys = None\n",
        "        self.split = 'train' if split == 0 else 'valid' if split == 1 else 'test'\n",
        "        self.h5py2int = lambda x: int(np.array(x))\n",
        "\n",
        "    def __len__(self):\n",
        "        f = h5py.File(self.datasetFile, 'r')\n",
        "        self.dataset_keys = [str(k) for k in f[self.split].keys()]\n",
        "        length = len(f[self.split])\n",
        "        f.close()\n",
        "\n",
        "        return length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.dataset is None:\n",
        "            self.dataset = h5py.File(self.datasetFile, mode='r')\n",
        "            self.dataset_keys = [str(k) for k in self.dataset[self.split].keys()]\n",
        "\n",
        "        example_name = self.dataset_keys[idx]\n",
        "        example = self.dataset[self.split][example_name]\n",
        "\n",
        "        # pdb.set_trace()\n",
        "\n",
        "        right_image = bytes(np.array(example['img']))\n",
        "        right_embed = np.array(example['embeddings'], dtype=float)\n",
        "        wrong_image = bytes(np.array(self.find_wrong_image(example['class'])))\n",
        "        inter_embed = np.array(self.find_inter_embed())\n",
        "\n",
        "        right_image = Image.open(io.BytesIO(right_image)).resize((64, 64))\n",
        "        wrong_image = Image.open(io.BytesIO(wrong_image)).resize((64, 64))\n",
        "\n",
        "        right_image = self.validate_image(right_image)\n",
        "        wrong_image = self.validate_image(wrong_image)\n",
        "\n",
        "        txt = np.array(example['txt']).astype(str)\n",
        "        class_ = np.array(example['class']).astype(str)\n",
        "\n",
        "        sample = {\n",
        "                'right_images': torch.FloatTensor(right_image),\n",
        "                'right_embed': torch.FloatTensor(right_embed),\n",
        "                'wrong_images': torch.FloatTensor(wrong_image),\n",
        "                'inter_embed': torch.FloatTensor(inter_embed),\n",
        "                'txt': str(txt),\n",
        "                'class': str(class_)\n",
        "                 }\n",
        "\n",
        "        sample['right_images'] = sample['right_images'].sub_(127.5).div_(127.5)\n",
        "        sample['wrong_images'] =sample['wrong_images'].sub_(127.5).div_(127.5)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def find_wrong_image(self, category):\n",
        "        idx = np.random.randint(len(self.dataset_keys))\n",
        "        example_name = self.dataset_keys[idx]\n",
        "        example = self.dataset[self.split][example_name]\n",
        "        _category = example['class']\n",
        "\n",
        "        if _category != category:\n",
        "            return example['img']\n",
        "\n",
        "        return self.find_wrong_image(category)\n",
        "\n",
        "    def find_inter_embed(self):\n",
        "        idx = np.random.randint(len(self.dataset_keys))\n",
        "        example_name = self.dataset_keys[idx]\n",
        "        example = self.dataset[self.split][example_name]\n",
        "        return example['embeddings']\n",
        "\n",
        "\n",
        "    def validate_image(self, img):\n",
        "        img = np.array(img, dtype=float)\n",
        "        if len(img.shape) < 3:\n",
        "            rgb = np.empty((64, 64, 3), dtype=np.float32)\n",
        "            rgb[:, :, 0] = img\n",
        "            rgb[:, :, 1] = img\n",
        "            rgb[:, :, 2] = img\n",
        "            img = rgb\n",
        "\n",
        "        return img.transpose(2, 0, 1)\n",
        "\n",
        "################ txt2image_dataset.py ends here ###################\n"
      ],
      "metadata": {
        "id": "kAkAXASftkzS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Concat_embed(nn.Module):\n",
        "    def __init__(self, embed_dim, projected_embed_dim):\n",
        "        super(Concat_embed, self).__init__()\n",
        "        self.projection = nn.Sequential(nn.Linear(in_features=embed_dim, out_features=projected_embed_dim),\n",
        "                                        nn.BatchNorm1d(num_features=projected_embed_dim),\n",
        "                                        nn.LeakyReLU(negative_slope=0.2, inplace=True))\n",
        "\n",
        "    def forward(self, inp, embed):\n",
        "        projected_embed = self.projection(embed)\n",
        "        replicated_embed = projected_embed.repeat(4, 4, 1, 1).permute(2, 3, 0, 1)\n",
        "        hidden_concat = torch.cat([inp, replicated_embed], 1)\n",
        "\n",
        "        return hidden_concat\n",
        "\n",
        "class Utils(object):\n",
        "    def __init__(self, cuda):\n",
        "        self.is_cuda = cuda\n",
        "\n",
        "    def cuda(self, variable):\n",
        "        return variable.cuda() if self.is_cuda else variable\n",
        "\n",
        "    @staticmethod\n",
        "    def smooth_label(tensor, offset):\n",
        "        return tensor + offset\n",
        "\n",
        "    @staticmethod\n",
        "    def save_checkpoint(netD, netG, dir_path, epoch):\n",
        "        path = dir_path #os.path.join(dir_path, subdir_path)\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "\n",
        "        torch.save(netD.state_dict(), '{0}/disc_{1}.pth'.format(path, epoch))\n",
        "        torch.save(netG.state_dict(), '{0}/gen_{1}.pth'.format(path, epoch))\n",
        "\n",
        "    @staticmethod\n",
        "    def weights_init(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('Conv') != -1:\n",
        "            m.weight.data.normal_(0.0, 0.02)\n",
        "        elif classname.find('BatchNorm') != -1:\n",
        "            m.weight.data.normal_(1.0, 0.02)\n",
        "            m.bias.data.fill_(0)\n",
        "\n",
        "################ utils.py ends here ###################"
      ],
      "metadata": {
        "id": "grFpuuEPtm6B"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class vae_encoder_generator(nn.Module):\n",
        "    def __init__(self, ngf):\n",
        "        super(vae_encoder_generator, self).__init__()\n",
        "        self.vae_encoder = vae_encoder(ngf)\n",
        "        self.vae_generator = vae_generator(ngf)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        x = self.vae_encoder(inp)\n",
        "        x = self.vae_generator(x)\n",
        "        return x\n",
        "\n",
        "    def generator_only(self, latent):\n",
        "        return self.vae_generator(latent)\n",
        "\n",
        "    def encoder_only(self, inp):\n",
        "        return self.vae_encoder(inp.cuda())\n",
        "\n",
        "\n",
        "class vae_generator(nn.Module):\n",
        "    def __init__(self, ngf):\n",
        "        super(vae_generator, self).__init__()\n",
        "        self.image_size = 64\n",
        "        self.num_channels = 3\n",
        "        self.noise_dim = 100\n",
        "        self.embed_dim = 1024\n",
        "        self.projected_embed_dim = 128\n",
        "        self.latent_dim = self.noise_dim + self.projected_embed_dim\n",
        "        self.ngf = ngf\n",
        "\n",
        "        # self.projection = nn.Sequential(nn.Linear(in_features=self.embed_dim, out_features=self.projected_embed_dim),\n",
        "        #     nn.BatchNorm1d(num_features=self.projected_embed_dim), nn.LeakyReLU(negative_slope=0.2, inplace=True))\n",
        "\n",
        "        # based on: https://github.com/pytorch/examples/blob/master/dcgan/main.py\n",
        "        self.netG = nn.Sequential(\n",
        "            nn.ConvTranspose2d(self.latent_dim, self.ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf * 8),\n",
        "            nn.ReLU(True), # state size. (ngf*8) x 4 x 4\n",
        "\n",
        "            nn.ConvTranspose2d(self.ngf * 8, self.ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf * 4),\n",
        "            nn.ReLU(True), # state size. (ngf*4) x 8 x 8\n",
        "\n",
        "            nn.ConvTranspose2d(self.ngf * 4, self.ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf * 2),\n",
        "            nn.ReLU(True), # state size. (ngf*2) x 16 x 16\n",
        "\n",
        "            nn.ConvTranspose2d(self.ngf * 2, self.ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d(self.ngf, self.num_channels, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (num_channels) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, latent_vector):\n",
        "        # projected_embed = self.projection(embed_vector).unsqueeze(2).unsqueeze(3)\n",
        "        # latent_vector = torch.cat([projected_embed, z], 1)\n",
        "        latent_vector = latent_vector.view(-1, self.latent_dim, 1, 1)\n",
        "        #print(\"**** latent_vector is cuda = \",latent_vector.cpu().is_cuda)\n",
        "        output = self.netG(latent_vector.cpu())\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class vae_encoder(nn.Module):\n",
        "    def __init__(self, ngf):\n",
        "        super(vae_encoder, self).__init__()\n",
        "        self.image_size = 64\n",
        "        self.num_channels = 3\n",
        "        self.embed_dim = 1024\n",
        "        self.noise_dim = 100\n",
        "        self.projected_embed_dim = 128\n",
        "        self.latent_dim = self.noise_dim + self.projected_embed_dim\n",
        "        self.ngf = ngf\n",
        "\n",
        "        self.projection = nn.Sequential(nn.Linear(in_features=self.embed_dim, out_features=self.projected_embed_dim),\n",
        "            nn.BatchNorm1d(num_features=self.projected_embed_dim), nn.LeakyReLU(negative_slope=0.2, inplace=True))\n",
        "\n",
        "        # based on: https://github.com/pytorch/examples/blob/master/dcgan/main.py\n",
        "        self.netE = nn.Sequential(\n",
        "            # state size. (num_channels) x 64 x 64\n",
        "            nn.Conv2d(self.num_channels, self.ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.Conv2d(self.ngf, self.ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.Conv2d(self.ngf * 2, self.ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.Conv2d(self.ngf * 4, self.ngf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.Conv2d(self.ngf * 8, self.latent_dim, 4, 1, 0, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, images):\n",
        "        output = self.netE(images)\n",
        "        #print(output.is_cuda)\n",
        "        return output\n",
        "\n",
        "\n",
        "class vae_discriminator(nn.Module):\n",
        "    def __init__(self, remove_noise):\n",
        "        super(discriminator, self).__init__()\n",
        "        self.image_size = 64\n",
        "        self.num_channels = 128\n",
        "        self.embed_dim = 1024\n",
        "        if remove_noise:\n",
        "            self.projected_embed_dim = 228\n",
        "            self.noise_dim = 0\n",
        "        else:\n",
        "            self.projected_embed_dim = 128\n",
        "            self.noise_dim = 100\n",
        "        self.B_dim = 128\n",
        "        self.C_dim = 16\n",
        "        self.minibatch_discriminator = minibatch_discriminator(self.num_channels, self.B_dim, self.C_dim)\n",
        "        #\n",
        "        self.netD_1 = nn.Sequential(\n",
        "            nn.Linear(self.projected_embed_dim + self.noise_dim, 228),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(228, 128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "        self.netD_2 = nn.Sequential(\n",
        "            nn.Linear(128 + self.B_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, inp):\n",
        "        x = inp.view(-1, self.projected_embed_dim + self.noise_dim)\n",
        "        x = self.netD_1(x)\n",
        "        x = self.minibatch_discriminator(x)\n",
        "        x = self.netD_2(x)\n",
        "\n",
        "        return x.view(-1)\n",
        "\n",
        "################ vae.py ends here ###################\n"
      ],
      "metadata": {
        "id": "xx3hnGu9tvwG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################ gan_cls.py ###################\n",
        "\n",
        "class generator(nn.Module):\n",
        "    def __init__(self, remove_noise, variational):\n",
        "        super(generator, self).__init__()\n",
        "        self.image_size = 64\n",
        "        self.num_channels = 3\n",
        "        self.embed_dim = 1024\n",
        "        self.remove_noise = remove_noise\n",
        "        if remove_noise:\n",
        "            self.noise_dim = 0\n",
        "            self.projected_embed_dim = 228\n",
        "        else:\n",
        "            self.noise_dim = 100\n",
        "            self.projected_embed_dim = 128\n",
        "        self.latent_dim = self.noise_dim + self.projected_embed_dim\n",
        "        self.ngf = 64\n",
        "        self.variational = variational\n",
        "        self.mu = None\n",
        "        self.sd = None\n",
        "\n",
        "        self.projection = nn.Sequential(nn.Linear(in_features=self.embed_dim, out_features=self.projected_embed_dim),\n",
        "            nn.BatchNorm1d(num_features=self.projected_embed_dim), nn.LeakyReLU(negative_slope=0.2, inplace=True))\n",
        "\n",
        "        if variational:\n",
        "            self.en_mu = nn.Conv2d(self.projected_embed_dim, self.projected_embed_dim, 1, 1, 0)\n",
        "            self.en_sigma = nn.Conv2d(self.projected_embed_dim, self.projected_embed_dim, 1, 1, 0)\n",
        "            self.softplus = nn.Softplus()\n",
        "            self.en_mu.weight.data.normal_(0, 0.002)\n",
        "            self.en_mu.bias.data.normal_(0, 0.002)\n",
        "            self.en_sigma.weight.data.normal_(0, 0.002)\n",
        "            self.en_sigma.bias.data.normal_(0, 0.002)\n",
        "\n",
        "        # based on: https://github.com/pytorch/examples/blob/master/dcgan/main.py\n",
        "        self.netG = nn.Sequential(nn.ConvTranspose2d(self.latent_dim, self.ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf * 8), nn.ReLU(True), # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(self.ngf * 8, self.ngf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(self.ngf * 4),\n",
        "            nn.ReLU(True), # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(self.ngf * 4, self.ngf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(self.ngf * 2),\n",
        "            nn.ReLU(True), # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(self.ngf * 2, self.ngf, 4, 2, 1, bias=False), nn.BatchNorm2d(self.ngf), nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d(self.ngf, self.num_channels, 4, 2, 1, bias=False), nn.Tanh()\n",
        "            # state size. (num_channels) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, embed_vector, z, noise):\n",
        "        return self.netG(self.encoder_only(embed_vector, z, noise))\n",
        "\n",
        "    def encoder_only(self, embed_vector, z, noise):\n",
        "        projected_embed = self.projection(embed_vector).unsqueeze(2).unsqueeze(3)\n",
        "        if self.variational:\n",
        "            self.mu = self.en_mu(projected_embed)\n",
        "            self.sd = self.softplus(self.en_sigma(projected_embed))\n",
        "            projected_embed = self.mu + self.sd.mul(noise)\n",
        "        if self.remove_noise:\n",
        "            latent_vector = projected_embed\n",
        "        else:\n",
        "            latent_vector = torch.cat([projected_embed, z], 1)\n",
        "        return latent_vector\n",
        "\n",
        "    def generator_only(self, latent_vector):\n",
        "        return self.netG(latent_vector)\n",
        "\n",
        "\n",
        "class discriminator(nn.Module):\n",
        "    def __init__(self, remove_noise):\n",
        "        super(discriminator, self).__init__()\n",
        "        self.image_size = 64\n",
        "        self.num_channels = 3\n",
        "        self.embed_dim = 1024\n",
        "        if remove_noise:\n",
        "            self.projected_embed_dim = 228\n",
        "        else:\n",
        "            self.projected_embed_dim = 128\n",
        "        self.ndf = 64\n",
        "        self.B_dim = 128\n",
        "        self.C_dim = 16\n",
        "\n",
        "        self.netD_1 = nn.Sequential(# input is (nc) x 64 x 64\n",
        "            nn.Conv2d(self.num_channels, self.ndf, 4, 2, 1, bias=False), nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(self.ndf, self.ndf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(self.ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True), # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(self.ndf * 2, self.ndf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(self.ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True), # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(self.ndf * 4, self.ndf * 8, 4, 2, 1, bias=False), nn.BatchNorm2d(self.ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True), )\n",
        "\n",
        "        self.projector = Concat_embed(self.embed_dim, self.projected_embed_dim)\n",
        "\n",
        "        self.netD_2 = nn.Sequential(# state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(self.ndf * 8 + self.projected_embed_dim, 1, 4, 1, 0, bias=False), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, inp, embed):\n",
        "        x_intermediate = self.netD_1(inp)\n",
        "        x = self.projector(x_intermediate, embed)\n",
        "        x = self.netD_2(x)\n",
        "\n",
        "        return x.view(-1, 1).squeeze(1), x_intermediate\n",
        "\n",
        "################ gan_cls.py ends here ###################\n"
      ],
      "metadata": {
        "id": "TQLQVtRTt7b0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################ gan_factory.py ###################\n",
        "\n",
        "class gan_factory(object):\n",
        "    @staticmethod\n",
        "    def generator_factory(type, ngf, remove_noise, variational=False):\n",
        "        if type == 'gan':\n",
        "            return generator(remove_noise, variational)\n",
        "        elif type == 'vae':\n",
        "            return vae_encoder_generator(ngf)\n",
        "\n",
        "    @staticmethod\n",
        "    def discriminator_factory(type, remove_noise):\n",
        "        if type == 'gan':\n",
        "            return discriminator(remove_noise)\n",
        "        elif type == 'vae':\n",
        "            return vae_discriminator(remove_noise)\n",
        "\n",
        "################ gan_factory.py ends here ###################"
      ],
      "metadata": {
        "id": "aZcTHfBXuCrI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, type, dataset, split, lr, diter, vis_screen, save_path, l1_coef, l2_coef, pre_trained_gen,\n",
        "                 pre_trained_disc, batch_size, num_workers, epochs, args, params_search=False):\n",
        "        self.config = args\n",
        "        self.cuda = torch.cuda.is_available()\n",
        "\n",
        "        self.generator = gan_factory.generator_factory(type, args.ngf, args.remove_noise_2, args.variational)\n",
        "        self.discriminator = gan_factory.discriminator_factory(type, args.remove_noise_2)\n",
        "\n",
        "        self.target_generator = gan_factory.generator_factory(args.target_type, args.ngf, args.remove_noise_2)\n",
        "\n",
        "        if self.cuda:\n",
        "            self.generator = self.generator.cuda()\n",
        "            self.discriminator = self.discriminator.cuda()\n",
        "\n",
        "        if pre_trained_disc:\n",
        "            print('loading {} from {}'.format('discriminator', pre_trained_disc))\n",
        "            self.discriminator.load_state_dict(torch.load(pre_trained_disc))\n",
        "        else:\n",
        "            if not params_search:\n",
        "                print('creating fresh params for {}'.format('discriminator'))\n",
        "            self.discriminator.apply(Utils.weights_init)\n",
        "\n",
        "        if pre_trained_gen:\n",
        "            print('loading {} from {}'.format('generator', pre_trained_gen))\n",
        "            self.generator.load_state_dict(torch.load(pre_trained_gen))\n",
        "        else:\n",
        "            if not params_search:\n",
        "                print('creating fresh params for {}'.format('generator'))\n",
        "            self.generator.apply(Utils.weights_init)\n",
        "\n",
        "        if dataset == 'flowers_only':\n",
        "            self.dataset = Text2ImageDataset(self.config.flowers_dataset_path, split=0)\n",
        "            self.target_dataset = Text2ImageDataset(self.config.flowers_dataset_path, split=2)\n",
        "        else:\n",
        "            print('Dataset not supported, please select either birds, flowers or flowers_only.')\n",
        "            exit()\n",
        "\n",
        "        self.noise_dim = 100\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.lr = lr\n",
        "        self.beta1 = 0.5\n",
        "        self.num_epochs = epochs\n",
        "        self.DITER = diter\n",
        "\n",
        "        self.l1_coef = l1_coef\n",
        "        self.l2_coef = l2_coef\n",
        "\n",
        "        self.data_loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True,\n",
        "                                      num_workers=self.num_workers)\n",
        "        self.target_data_loader = DataLoader(self.target_dataset, batch_size=self.batch_size, shuffle=True,\n",
        "                                             num_workers=self.num_workers)\n",
        "\n",
        "        self.optimD = torch.optim.Adam(self.discriminator.parameters(), lr=self.lr, betas=(self.beta1, 0.999))\n",
        "        self.optimG = torch.optim.Adam(self.generator.parameters(), lr=self.lr, betas=(self.beta1, 0.999))\n",
        "\n",
        "        self.save_path = save_path\n",
        "        self.type = type\n",
        "        # self.h_el = args.h_el\n",
        "        self.args = args\n",
        "        if not params_search:\n",
        "            self.checkpoints_path = 'tmp/'\n",
        "            if not os.path.exists(self.checkpoints_path):\n",
        "                os.makedirs(self.checkpoints_path)\n",
        "            print(\"***Calling Nearest Neighbour***\")\n",
        "            self.nn = NearestNeighbor(self.target_data_loader, dataset, self.cuda, args.ngf)\n",
        "        self.params_search = params_search\n",
        "\n",
        "    def train(self, cls=False):\n",
        "        print(\"*** Inside train() func ***\")\n",
        "        if self.type == 'gan':\n",
        "            self._train_gan(cls)\n",
        "\n",
        "    def _train_gan(self, cls):\n",
        "        print(\"*** Inside _train_gan() func ***\")\n",
        "        criterion = nn.BCELoss()\n",
        "        l2_loss = nn.MSELoss()\n",
        "        l1_loss = nn.L1Loss()\n",
        "        iteration = 0\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            for sample in self.data_loader:\n",
        "                iteration += 1\n",
        "                right_images = sample['right_images']\n",
        "                right_embed = sample['right_embed']\n",
        "                wrong_images = sample['wrong_images']\n",
        "\n",
        "                right_images = Variable(right_images.float()).cuda()\n",
        "                right_embed = Variable(right_embed.float()).cuda()\n",
        "                wrong_images = Variable(wrong_images.float()).cuda()\n",
        "\n",
        "                real_labels = torch.ones(right_images.size(0))\n",
        "                fake_labels = torch.zeros(right_images.size(0))\n",
        "\n",
        "                # ======== One sided label smoothing ==========\n",
        "                # Helps preventing the discriminator from overpowering the\n",
        "                # generator adding penalty when the discriminator is too confident\n",
        "                # =============================================\n",
        "                smoothed_real_labels = torch.FloatTensor(Utils.smooth_label(real_labels.numpy(), -0.1))\n",
        "\n",
        "                real_labels = Variable(real_labels).cuda()\n",
        "                smoothed_real_labels = Variable(smoothed_real_labels).cuda()\n",
        "                fake_labels = Variable(fake_labels).cuda()\n",
        "\n",
        "                # Train the discriminator\n",
        "                self.discriminator.zero_grad()\n",
        "                outputs, activation_real = self.discriminator(right_images, right_embed)\n",
        "                real_loss = criterion(outputs, smoothed_real_labels)\n",
        "                real_score = outputs\n",
        "\n",
        "                if cls:\n",
        "                    outputs, _ = self.discriminator(wrong_images, right_embed)\n",
        "                    wrong_loss = criterion(outputs, fake_labels)\n",
        "                    wrong_score = outputs\n",
        "\n",
        "                if self.args.remove_noise:\n",
        "                    noise = Variable(torch.zeros(right_images.size(0), self.noise_dim)).cuda()\n",
        "                else:\n",
        "                    noise = Variable(torch.randn(right_images.size(0), self.noise_dim)).cuda()\n",
        "                noise = noise.view(noise.size(0), 100, 1, 1)\n",
        "                fake_images = self.generator(right_embed, noise, noise)\n",
        "                outputs, _ = self.discriminator(fake_images, right_embed)\n",
        "                fake_loss = criterion(outputs, fake_labels)\n",
        "                fake_score = outputs\n",
        "\n",
        "                d_loss = real_loss + fake_loss\n",
        "\n",
        "                if cls:\n",
        "                    d_loss = d_loss + wrong_loss\n",
        "\n",
        "                d_loss.backward()\n",
        "                self.optimD.step()\n",
        "\n",
        "                # Train the generator\n",
        "                self.generator.zero_grad()\n",
        "                if self.args.remove_noise:\n",
        "                    noise = Variable(torch.zeros(right_images.size(0), self.noise_dim)).cuda()\n",
        "                else:\n",
        "                    noise = Variable(torch.randn(right_images.size(0), self.noise_dim)).cuda()\n",
        "                noise = noise.view(noise.size(0), 100, 1, 1)\n",
        "                fake_images = self.generator(right_embed, noise, noise)\n",
        "                outputs, activation_fake = self.discriminator(fake_images, right_embed)\n",
        "                _, activation_real = self.discriminator(right_images, right_embed)\n",
        "\n",
        "                activation_fake = torch.mean(activation_fake, 0)\n",
        "                activation_real = torch.mean(activation_real, 0)\n",
        "                # ======= Generator Loss function============\n",
        "                # This is a customized loss function, the first term is the regular cross entropy loss\n",
        "                # The second term is feature matching loss, this measure the distance between the real and generated\n",
        "                # images statistics by comparing intermediate layers activations\n",
        "                # The third term is L1 distance between the generated and real images, this is helpful for the conditional case\n",
        "                # because it links the embedding feature vector directly to certain pixel values.\n",
        "                # ===========================================\n",
        "                g_loss = criterion(outputs, real_labels) +\\\n",
        "                         self.l2_coef * l2_loss(activation_fake, activation_real.detach()) +\\\n",
        "                         self.l1_coef * l1_loss(fake_images, right_images)\n",
        "\n",
        "                g_loss.backward()\n",
        "                self.optimG.step()\n",
        "\n",
        "                if iteration % 10 == 0:\n",
        "                    print(\"Epoch: %d, d_loss= %f, g_loss= %f, ccaD(X)= %f, D(G(X))= %f\" % (epoch, d_loss.data.cpu().mean(), g_loss.data.cpu().mean(), real_score.data.cpu().mean(), fake_score.data.cpu().mean()))\n",
        "\n",
        "            if (epoch) % 10 == 0:\n",
        "                Utils.save_checkpoint(self.discriminator, self.generator, self.save_path, epoch)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MYWIbmsJuF-s"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oeAAQXBQuSnW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}