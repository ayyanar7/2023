{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNyaWHotwuZ/WlmjcfETXSZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayyanar7/2023/blob/main/Text2Imagegen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dRFaarbGM465",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1ee824c-8fda-42f9-cd11-7a2f0210a905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/flowershd5dataset\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "# import kagglehub\n",
        "# ds_path = kagglehub.dataset_download('kmahesh541/flowershd5dataset')\n",
        "# words_path = kagglehub.dataset_download('msripooja/flowershd5words')\n",
        "# mast_d_path = kagglehub.dataset_download('kaushalmak07/mast-d')\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"kmahesh541/flowershd5dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print('Dataset path.',ds_path)\n",
        "# print('words path.',words_path)\n",
        "# print('mast path.',mast_d_path)\n",
        "hdf5_fpath = path+\"/flowers-hd5/data/flowers/flowers.hdf5\"\n",
        "print(hdf5_fpath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g64mkDOKrbdw",
        "outputId": "6de9a686-ec6b-4c33-f2ec-f446a608dfd0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/kaggle/input/flowershd5dataset/flowers-hd5/data/flowers/flowers.hdf5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import io\n",
        "import h5py\n",
        "import argparse\n",
        "import torch\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torch import nn\n",
        "from datetime import timedelta\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import os\n",
        "print(\"All libraries imported!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44UTxyM_skvJ",
        "outputId": "ed335c76-f2da-4fc4-dd8a-e1980ca53694"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = h5py.File(hdf5_fpath)\n",
        "\n",
        "#1. to know the categories in hdf5 file\n",
        "print(list(f))\n",
        "print(\"\\nNo. of items in test = \",len(list(f['test'])))\n",
        "print(\"\\nNo. of items in train = \",len(list(f['train'])))\n",
        "print(\"\\nNo. of items in valid = \",len(list(f['valid'])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPcZ-3jLtGNd",
        "outputId": "5d55fffa-0af5-4252-ce41-848a849f127d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['test', 'train', 'valid']\n",
            "\n",
            "No. of items in test =  5775\n",
            "\n",
            "No. of items in train =  29390\n",
            "\n",
            "No. of items in valid =  5780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NearestNeighbor:\n",
        "    def __init__(self, dataset, source, cuda, ngf):\n",
        "        self.dataset = dataset\n",
        "        data = None\n",
        "        representation = None\n",
        "        labels = []\n",
        "        embeddings = []\n",
        "        path = ''\n",
        "        data_path = path + 'source_{}_nn_data.pl'.format(source)\n",
        "        labels_path = path + 'source_{}_nn_labels.pl'.format(source)\n",
        "        nbrs_path = path + 'source_{}_nn.pl'.format(source)\n",
        "        embeddings_path = path + 'source_{}_nn_embeddings.pl'.format(source)\n",
        "        print(\"data_path: \",data_path)\n",
        "        print(\"data_path: \"+labels_path)\n",
        "        print(\"data_path: \"+nbrs_path)\n",
        "        print(\"data_path: \"+embeddings_path)\n",
        "        self.model = gan_factory.generator_factory('vae', ngf, False)\n",
        "        if cuda:\n",
        "            self.model = self.model.cuda()\n",
        "        #self.model.load_state_dict(torch.load('./checkpoints/flowers_autoencoder/gen.pth'))\n",
        "\n",
        "        if os.path.exists(data_path):\n",
        "            print('start loading data for NN test {}'.format(data_path))\n",
        "            data = pickle.load(open(data_path, 'rb'))\n",
        "            labels = pickle.load(open(labels_path, 'rb'))\n",
        "            nbrs = pickle.load(open(nbrs_path, 'rb'))\n",
        "            embeddings = pickle.load(open(embeddings_path, 'rb'))\n",
        "        else:\n",
        "            print('start creating data for NN test {}'.format(data_path))\n",
        "            for i, sample in enumerate(dataset):\n",
        "                #print(\"**** iter i = \",i)\n",
        "                if data is None:\n",
        "                    data = sample['right_images'].numpy()\n",
        "                    data_var = Variable(sample['right_images'].float(), volatile=True)\n",
        "                    if cuda:\n",
        "                        data_var = data_var.cuda()\n",
        "                    representation = self.model.encoder_only(data_var).data.cpu().numpy()\n",
        "                    labels = sample['txt']\n",
        "                    embeddings = sample['right_embed']\n",
        "                else:\n",
        "                    data = np.append(data, sample['right_images'].numpy(), axis=0)\n",
        "                    data_var = Variable(sample['right_images'].float(), volatile=True)\n",
        "                    if cuda:\n",
        "                        data_var = data_var.cuda()\n",
        "                    representation = np.append(representation, self.model.encoder_only(data_var).data.cpu().numpy(),\n",
        "                                               axis=0)\n",
        "                    labels += sample['txt']\n",
        "                    embeddings = np.append(embeddings, sample['right_embed'].numpy(), axis=0)\n",
        "            nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(representation.reshape(-1, 228))\n",
        "            pickle.dump(data, open(data_path, 'wb'))\n",
        "            pickle.dump(labels, open(labels_path, 'wb'))\n",
        "            pickle.dump(nbrs, open(nbrs_path, 'wb'))\n",
        "            pickle.dump(embeddings, open(embeddings_path, 'wb'))\n",
        "        print('finish loading data for NN test')\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.nbrs = nbrs\n",
        "        self.embeddings = embeddings\n",
        "\n",
        "    def get_text(self, samples, limit=-1):\n",
        "        text_results, _, _ = self.get_text_and_images_and_embedding(samples, limit)\n",
        "        return text_results\n",
        "\n",
        "    def get_text_and_images(self, samples, limit):\n",
        "        text_results, image_results, _ = self.get_text_and_images_and_embedding(samples, limit)\n",
        "        return text_results, image_results\n",
        "\n",
        "    def get_text_and_images_and_embedding(self, samples, limit=-1):\n",
        "        samples_embedding = self.model.encoder_only(samples).data.cpu().numpy().reshape(-1, 228)\n",
        "        if limit != -1:\n",
        "            samples_embedding = samples_embedding[:limit]\n",
        "        distances, indices = self.nbrs.kneighbors(samples_embedding)\n",
        "        text_results = [self.labels[index] for index in indices[:, 0]]\n",
        "        image_results = [self.data[index] for index in indices[:, 0]]\n",
        "        embedding_results = [self.embeddings[index] for index in indices[:, 0]]\n",
        "        return text_results, image_results, embedding_results"
      ],
      "metadata": {
        "id": "JZBQG45wtZMM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################ txt2image_dataset.py ###################\n",
        "\n",
        "class Text2ImageDataset(Dataset):\n",
        "\n",
        "    def __init__(self, datasetFile, transform=None, split=0):\n",
        "        self.datasetFile = datasetFile\n",
        "        self.transform = transform\n",
        "        self.dataset = None\n",
        "        self.dataset_keys = None\n",
        "        self.split = 'train' if split == 0 else 'valid' if split == 1 else 'test'\n",
        "        self.h5py2int = lambda x: int(np.array(x))\n",
        "\n",
        "    def __len__(self):\n",
        "        f = h5py.File(self.datasetFile, 'r')\n",
        "        self.dataset_keys = [str(k) for k in f[self.split].keys()]\n",
        "        length = len(f[self.split])\n",
        "        f.close()\n",
        "\n",
        "        return length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.dataset is None:\n",
        "            self.dataset = h5py.File(self.datasetFile, mode='r')\n",
        "            self.dataset_keys = [str(k) for k in self.dataset[self.split].keys()]\n",
        "\n",
        "        example_name = self.dataset_keys[idx]\n",
        "        example = self.dataset[self.split][example_name]\n",
        "\n",
        "        # pdb.set_trace()\n",
        "\n",
        "        right_image = bytes(np.array(example['img']))\n",
        "        right_embed = np.array(example['embeddings'], dtype=float)\n",
        "        wrong_image = bytes(np.array(self.find_wrong_image(example['class'])))\n",
        "        inter_embed = np.array(self.find_inter_embed())\n",
        "\n",
        "        right_image = Image.open(io.BytesIO(right_image)).resize((64, 64))\n",
        "        wrong_image = Image.open(io.BytesIO(wrong_image)).resize((64, 64))\n",
        "\n",
        "        right_image = self.validate_image(right_image)\n",
        "        wrong_image = self.validate_image(wrong_image)\n",
        "\n",
        "        txt = np.array(example['txt']).astype(str)\n",
        "        class_ = np.array(example['class']).astype(str)\n",
        "\n",
        "        sample = {\n",
        "                'right_images': torch.FloatTensor(right_image),\n",
        "                'right_embed': torch.FloatTensor(right_embed),\n",
        "                'wrong_images': torch.FloatTensor(wrong_image),\n",
        "                'inter_embed': torch.FloatTensor(inter_embed),\n",
        "                'txt': str(txt),\n",
        "                'class': str(class_)\n",
        "                 }\n",
        "\n",
        "        sample['right_images'] = sample['right_images'].sub_(127.5).div_(127.5)\n",
        "        sample['wrong_images'] =sample['wrong_images'].sub_(127.5).div_(127.5)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def find_wrong_image(self, category):\n",
        "        idx = np.random.randint(len(self.dataset_keys))\n",
        "        example_name = self.dataset_keys[idx]\n",
        "        example = self.dataset[self.split][example_name]\n",
        "        _category = example['class']\n",
        "\n",
        "        if _category != category:\n",
        "            return example['img']\n",
        "\n",
        "        return self.find_wrong_image(category)\n",
        "\n",
        "    def find_inter_embed(self):\n",
        "        idx = np.random.randint(len(self.dataset_keys))\n",
        "        example_name = self.dataset_keys[idx]\n",
        "        example = self.dataset[self.split][example_name]\n",
        "        return example['embeddings']\n",
        "\n",
        "\n",
        "    def validate_image(self, img):\n",
        "        img = np.array(img, dtype=float)\n",
        "        if len(img.shape) < 3:\n",
        "            rgb = np.empty((64, 64, 3), dtype=np.float32)\n",
        "            rgb[:, :, 0] = img\n",
        "            rgb[:, :, 1] = img\n",
        "            rgb[:, :, 2] = img\n",
        "            img = rgb\n",
        "\n",
        "        return img.transpose(2, 0, 1)\n",
        "\n",
        "################ txt2image_dataset.py ends here ###################\n"
      ],
      "metadata": {
        "id": "kAkAXASftkzS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Concat_embed(nn.Module):\n",
        "    def __init__(self, embed_dim, projected_embed_dim):\n",
        "        super(Concat_embed, self).__init__()\n",
        "        self.projection = nn.Sequential(nn.Linear(in_features=embed_dim, out_features=projected_embed_dim),\n",
        "                                        nn.BatchNorm1d(num_features=projected_embed_dim),\n",
        "                                        nn.LeakyReLU(negative_slope=0.2, inplace=True))\n",
        "\n",
        "    def forward(self, inp, embed):\n",
        "        projected_embed = self.projection(embed)\n",
        "        replicated_embed = projected_embed.repeat(4, 4, 1, 1).permute(2, 3, 0, 1)\n",
        "        hidden_concat = torch.cat([inp, replicated_embed], 1)\n",
        "\n",
        "        return hidden_concat\n",
        "\n",
        "class Utils(object):\n",
        "    def __init__(self, cuda):\n",
        "        self.is_cuda = cuda\n",
        "\n",
        "    def cuda(self, variable):\n",
        "        return variable.cuda() if self.is_cuda else variable\n",
        "\n",
        "    @staticmethod\n",
        "    def smooth_label(tensor, offset):\n",
        "        return tensor + offset\n",
        "\n",
        "    @staticmethod\n",
        "    def save_checkpoint(netD, netG, dir_path, epoch):\n",
        "        path = dir_path #os.path.join(dir_path, subdir_path)\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "\n",
        "        torch.save(netD.state_dict(), '{0}/disc_{1}.pth'.format(path, epoch))\n",
        "        torch.save(netG.state_dict(), '{0}/gen_{1}.pth'.format(path, epoch))\n",
        "\n",
        "    @staticmethod\n",
        "    def weights_init(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('Conv') != -1:\n",
        "            m.weight.data.normal_(0.0, 0.02)\n",
        "        elif classname.find('BatchNorm') != -1:\n",
        "            m.weight.data.normal_(1.0, 0.02)\n",
        "            m.bias.data.fill_(0)\n",
        "\n",
        "################ utils.py ends here ###################"
      ],
      "metadata": {
        "id": "grFpuuEPtm6B"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class vae_encoder_generator(nn.Module):\n",
        "    def __init__(self, ngf):\n",
        "        super(vae_encoder_generator, self).__init__()\n",
        "        self.vae_encoder = vae_encoder(ngf)\n",
        "        self.vae_generator = vae_generator(ngf)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        x = self.vae_encoder(inp)\n",
        "        x = self.vae_generator(x)\n",
        "        return x\n",
        "\n",
        "    def generator_only(self, latent):\n",
        "        return self.vae_generator(latent)\n",
        "\n",
        "    def encoder_only(self, inp):\n",
        "        return self.vae_encoder(inp.cuda())\n",
        "\n",
        "\n",
        "class vae_generator(nn.Module):\n",
        "    def __init__(self, ngf):\n",
        "        super(vae_generator, self).__init__()\n",
        "        self.image_size = 64\n",
        "        self.num_channels = 3\n",
        "        self.noise_dim = 100\n",
        "        self.embed_dim = 1024\n",
        "        self.projected_embed_dim = 128\n",
        "        self.latent_dim = self.noise_dim + self.projected_embed_dim\n",
        "        self.ngf = ngf\n",
        "\n",
        "        # self.projection = nn.Sequential(nn.Linear(in_features=self.embed_dim, out_features=self.projected_embed_dim),\n",
        "        #     nn.BatchNorm1d(num_features=self.projected_embed_dim), nn.LeakyReLU(negative_slope=0.2, inplace=True))\n",
        "\n",
        "        # based on: https://github.com/pytorch/examples/blob/master/dcgan/main.py\n",
        "        self.netG = nn.Sequential(\n",
        "            nn.ConvTranspose2d(self.latent_dim, self.ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf * 8),\n",
        "            nn.ReLU(True), # state size. (ngf*8) x 4 x 4\n",
        "\n",
        "            nn.ConvTranspose2d(self.ngf * 8, self.ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf * 4),\n",
        "            nn.ReLU(True), # state size. (ngf*4) x 8 x 8\n",
        "\n",
        "            nn.ConvTranspose2d(self.ngf * 4, self.ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf * 2),\n",
        "            nn.ReLU(True), # state size. (ngf*2) x 16 x 16\n",
        "\n",
        "            nn.ConvTranspose2d(self.ngf * 2, self.ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d(self.ngf, self.num_channels, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (num_channels) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, latent_vector):\n",
        "        # projected_embed = self.projection(embed_vector).unsqueeze(2).unsqueeze(3)\n",
        "        # latent_vector = torch.cat([projected_embed, z], 1)\n",
        "        latent_vector = latent_vector.view(-1, self.latent_dim, 1, 1)\n",
        "        #print(\"**** latent_vector is cuda = \",latent_vector.cpu().is_cuda)\n",
        "        output = self.netG(latent_vector.cpu())\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class vae_encoder(nn.Module):\n",
        "    def __init__(self, ngf):\n",
        "        super(vae_encoder, self).__init__()\n",
        "        self.image_size = 64\n",
        "        self.num_channels = 3\n",
        "        self.embed_dim = 1024\n",
        "        self.noise_dim = 100\n",
        "        self.projected_embed_dim = 128\n",
        "        self.latent_dim = self.noise_dim + self.projected_embed_dim\n",
        "        self.ngf = ngf\n",
        "\n",
        "        self.projection = nn.Sequential(nn.Linear(in_features=self.embed_dim, out_features=self.projected_embed_dim),\n",
        "            nn.BatchNorm1d(num_features=self.projected_embed_dim), nn.LeakyReLU(negative_slope=0.2, inplace=True))\n",
        "\n",
        "        # based on: https://github.com/pytorch/examples/blob/master/dcgan/main.py\n",
        "        self.netE = nn.Sequential(\n",
        "            # state size. (num_channels) x 64 x 64\n",
        "            nn.Conv2d(self.num_channels, self.ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.Conv2d(self.ngf, self.ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.Conv2d(self.ngf * 2, self.ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.Conv2d(self.ngf * 4, self.ngf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.Conv2d(self.ngf * 8, self.latent_dim, 4, 1, 0, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, images):\n",
        "        output = self.netE(images)\n",
        "        #print(output.is_cuda)\n",
        "        return output\n",
        "\n",
        "\n",
        "class vae_discriminator(nn.Module):\n",
        "    def __init__(self, remove_noise):\n",
        "        super(discriminator, self).__init__()\n",
        "        self.image_size = 64\n",
        "        self.num_channels = 128\n",
        "        self.embed_dim = 1024\n",
        "        if remove_noise:\n",
        "            self.projected_embed_dim = 228\n",
        "            self.noise_dim = 0\n",
        "        else:\n",
        "            self.projected_embed_dim = 128\n",
        "            self.noise_dim = 100\n",
        "        self.B_dim = 128\n",
        "        self.C_dim = 16\n",
        "        self.minibatch_discriminator = minibatch_discriminator(self.num_channels, self.B_dim, self.C_dim)\n",
        "        #\n",
        "        self.netD_1 = nn.Sequential(\n",
        "            nn.Linear(self.projected_embed_dim + self.noise_dim, 228),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(228, 128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "        self.netD_2 = nn.Sequential(\n",
        "            nn.Linear(128 + self.B_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, inp):\n",
        "        x = inp.view(-1, self.projected_embed_dim + self.noise_dim)\n",
        "        x = self.netD_1(x)\n",
        "        x = self.minibatch_discriminator(x)\n",
        "        x = self.netD_2(x)\n",
        "\n",
        "        return x.view(-1)\n",
        "\n",
        "################ vae.py ends here ###################\n"
      ],
      "metadata": {
        "id": "xx3hnGu9tvwG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################ gan_cls.py ###################\n",
        "\n",
        "class generator(nn.Module):\n",
        "    def __init__(self, remove_noise, variational):\n",
        "        super(generator, self).__init__()\n",
        "        self.image_size = 64\n",
        "        self.num_channels = 3\n",
        "        self.embed_dim = 1024\n",
        "        self.remove_noise = remove_noise\n",
        "        if remove_noise:\n",
        "            self.noise_dim = 0\n",
        "            self.projected_embed_dim = 228\n",
        "        else:\n",
        "            self.noise_dim = 100\n",
        "            self.projected_embed_dim = 128\n",
        "        self.latent_dim = self.noise_dim + self.projected_embed_dim\n",
        "        self.ngf = 64\n",
        "        self.variational = variational\n",
        "        self.mu = None\n",
        "        self.sd = None\n",
        "\n",
        "        self.projection = nn.Sequential(nn.Linear(in_features=self.embed_dim, out_features=self.projected_embed_dim),\n",
        "            nn.BatchNorm1d(num_features=self.projected_embed_dim), nn.LeakyReLU(negative_slope=0.2, inplace=True))\n",
        "\n",
        "        if variational:\n",
        "            self.en_mu = nn.Conv2d(self.projected_embed_dim, self.projected_embed_dim, 1, 1, 0)\n",
        "            self.en_sigma = nn.Conv2d(self.projected_embed_dim, self.projected_embed_dim, 1, 1, 0)\n",
        "            self.softplus = nn.Softplus()\n",
        "            self.en_mu.weight.data.normal_(0, 0.002)\n",
        "            self.en_mu.bias.data.normal_(0, 0.002)\n",
        "            self.en_sigma.weight.data.normal_(0, 0.002)\n",
        "            self.en_sigma.bias.data.normal_(0, 0.002)\n",
        "\n",
        "        # based on: https://github.com/pytorch/examples/blob/master/dcgan/main.py\n",
        "        self.netG = nn.Sequential(nn.ConvTranspose2d(self.latent_dim, self.ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf * 8), nn.ReLU(True), # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(self.ngf * 8, self.ngf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(self.ngf * 4),\n",
        "            nn.ReLU(True), # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(self.ngf * 4, self.ngf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(self.ngf * 2),\n",
        "            nn.ReLU(True), # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(self.ngf * 2, self.ngf, 4, 2, 1, bias=False), nn.BatchNorm2d(self.ngf), nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d(self.ngf, self.num_channels, 4, 2, 1, bias=False), nn.Tanh()\n",
        "            # state size. (num_channels) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, embed_vector, z, noise):\n",
        "        return self.netG(self.encoder_only(embed_vector, z, noise))\n",
        "\n",
        "    def encoder_only(self, embed_vector, z, noise):\n",
        "        projected_embed = self.projection(embed_vector).unsqueeze(2).unsqueeze(3)\n",
        "        if self.variational:\n",
        "            self.mu = self.en_mu(projected_embed)\n",
        "            self.sd = self.softplus(self.en_sigma(projected_embed))\n",
        "            projected_embed = self.mu + self.sd.mul(noise)\n",
        "        if self.remove_noise:\n",
        "            latent_vector = projected_embed\n",
        "        else:\n",
        "            latent_vector = torch.cat([projected_embed, z], 1)\n",
        "        return latent_vector\n",
        "\n",
        "    def generator_only(self, latent_vector):\n",
        "        return self.netG(latent_vector)\n",
        "\n",
        "\n",
        "class discriminator(nn.Module):\n",
        "    def __init__(self, remove_noise):\n",
        "        super(discriminator, self).__init__()\n",
        "        self.image_size = 64\n",
        "        self.num_channels = 3\n",
        "        self.embed_dim = 1024\n",
        "        if remove_noise:\n",
        "            self.projected_embed_dim = 228\n",
        "        else:\n",
        "            self.projected_embed_dim = 128\n",
        "        self.ndf = 64\n",
        "        self.B_dim = 128\n",
        "        self.C_dim = 16\n",
        "\n",
        "        self.netD_1 = nn.Sequential(# input is (nc) x 64 x 64\n",
        "            nn.Conv2d(self.num_channels, self.ndf, 4, 2, 1, bias=False), nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(self.ndf, self.ndf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(self.ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True), # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(self.ndf * 2, self.ndf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(self.ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True), # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(self.ndf * 4, self.ndf * 8, 4, 2, 1, bias=False), nn.BatchNorm2d(self.ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True), )\n",
        "\n",
        "        self.projector = Concat_embed(self.embed_dim, self.projected_embed_dim)\n",
        "\n",
        "        self.netD_2 = nn.Sequential(# state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(self.ndf * 8 + self.projected_embed_dim, 1, 4, 1, 0, bias=False), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, inp, embed):\n",
        "        x_intermediate = self.netD_1(inp)\n",
        "        x = self.projector(x_intermediate, embed)\n",
        "        x = self.netD_2(x)\n",
        "\n",
        "        return x.view(-1, 1).squeeze(1), x_intermediate\n",
        "\n",
        "################ gan_cls.py ends here ###################\n"
      ],
      "metadata": {
        "id": "TQLQVtRTt7b0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################ gan_factory.py ###################\n",
        "\n",
        "class gan_factory(object):\n",
        "    @staticmethod\n",
        "    def generator_factory(type, ngf, remove_noise, variational=False):\n",
        "        if type == 'gan':\n",
        "            return generator(remove_noise, variational)\n",
        "        elif type == 'vae':\n",
        "            return vae_encoder_generator(ngf)\n",
        "\n",
        "    @staticmethod\n",
        "    def discriminator_factory(type, remove_noise):\n",
        "        if type == 'gan':\n",
        "            return discriminator(remove_noise)\n",
        "        elif type == 'vae':\n",
        "            return vae_discriminator(remove_noise)\n",
        "\n",
        "################ gan_factory.py ends here ###################"
      ],
      "metadata": {
        "id": "aZcTHfBXuCrI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, type, dataset, split, lr, diter, vis_screen, save_path, l1_coef, l2_coef, pre_trained_gen,\n",
        "                 pre_trained_disc, batch_size, num_workers, epochs, args, params_search=False):\n",
        "        self.config = args\n",
        "        self.cuda = torch.cuda.is_available()\n",
        "\n",
        "        self.generator = gan_factory.generator_factory(type, args.ngf, args.remove_noise_2, args.variational)\n",
        "        self.discriminator = gan_factory.discriminator_factory(type, args.remove_noise_2)\n",
        "\n",
        "        self.target_generator = gan_factory.generator_factory(args.target_type, args.ngf, args.remove_noise_2)\n",
        "\n",
        "        if self.cuda:\n",
        "            self.generator = self.generator.cuda()\n",
        "            self.discriminator = self.discriminator.cuda()\n",
        "\n",
        "        if pre_trained_disc:\n",
        "            print('loading {} from {}'.format('discriminator', pre_trained_disc))\n",
        "            self.discriminator.load_state_dict(torch.load(pre_trained_disc))\n",
        "        else:\n",
        "            if not params_search:\n",
        "                print('creating fresh params for {}'.format('discriminator'))\n",
        "            self.discriminator.apply(Utils.weights_init)\n",
        "\n",
        "        if pre_trained_gen:\n",
        "            print('loading {} from {}'.format('generator', pre_trained_gen))\n",
        "            self.generator.load_state_dict(torch.load(pre_trained_gen))\n",
        "        else:\n",
        "            if not params_search:\n",
        "                print('creating fresh params for {}'.format('generator'))\n",
        "            self.generator.apply(Utils.weights_init)\n",
        "\n",
        "        if dataset == 'flowers_only':\n",
        "            self.dataset = Text2ImageDataset(self.config.flowers_dataset_path, split=0)\n",
        "            self.target_dataset = Text2ImageDataset(self.config.flowers_dataset_path, split=2)\n",
        "        else:\n",
        "            print('Dataset not supported, please select either birds, flowers or flowers_only.')\n",
        "            exit()\n",
        "\n",
        "        self.noise_dim = 100\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.lr = lr\n",
        "        self.beta1 = 0.5\n",
        "        self.num_epochs = epochs\n",
        "        self.DITER = diter\n",
        "\n",
        "        self.l1_coef = l1_coef\n",
        "        self.l2_coef = l2_coef\n",
        "\n",
        "        self.data_loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True,\n",
        "                                      num_workers=self.num_workers)\n",
        "        self.target_data_loader = DataLoader(self.target_dataset, batch_size=self.batch_size, shuffle=True,\n",
        "                                             num_workers=self.num_workers)\n",
        "\n",
        "        self.optimD = torch.optim.Adam(self.discriminator.parameters(), lr=self.lr, betas=(self.beta1, 0.999))\n",
        "        self.optimG = torch.optim.Adam(self.generator.parameters(), lr=self.lr, betas=(self.beta1, 0.999))\n",
        "\n",
        "        self.save_path = save_path\n",
        "        self.type = type\n",
        "        # self.h_el = args.h_el\n",
        "        self.args = args\n",
        "        if not params_search:\n",
        "            self.checkpoints_path = 'tmp/'\n",
        "            if not os.path.exists(self.checkpoints_path):\n",
        "                os.makedirs(self.checkpoints_path)\n",
        "            print(\"***Calling Nearest Neighbour***\")\n",
        "            self.nn = NearestNeighbor(self.target_data_loader, dataset, self.cuda, args.ngf)\n",
        "        self.params_search = params_search\n",
        "\n",
        "    def train(self, cls=False):\n",
        "        print(\"*** Inside train() func ***\")\n",
        "        if self.type == 'gan':\n",
        "            self._train_gan(cls)\n",
        "\n",
        "    def _train_gan(self, cls):\n",
        "        print(\"*** Inside _train_gan() func ***\")\n",
        "        criterion = nn.BCELoss()\n",
        "        l2_loss = nn.MSELoss()\n",
        "        l1_loss = nn.L1Loss()\n",
        "        iteration = 0\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            for sample in self.data_loader:\n",
        "                iteration += 1\n",
        "                right_images = sample['right_images']\n",
        "                right_embed = sample['right_embed']\n",
        "                wrong_images = sample['wrong_images']\n",
        "\n",
        "                right_images = Variable(right_images.float()).cuda()\n",
        "                right_embed = Variable(right_embed.float()).cuda()\n",
        "                wrong_images = Variable(wrong_images.float()).cuda()\n",
        "\n",
        "                real_labels = torch.ones(right_images.size(0))\n",
        "                fake_labels = torch.zeros(right_images.size(0))\n",
        "\n",
        "                # ======== One sided label smoothing ==========\n",
        "                # Helps preventing the discriminator from overpowering the\n",
        "                # generator adding penalty when the discriminator is too confident\n",
        "                # =============================================\n",
        "                smoothed_real_labels = torch.FloatTensor(Utils.smooth_label(real_labels.numpy(), -0.1))\n",
        "\n",
        "                real_labels = Variable(real_labels).cuda()\n",
        "                smoothed_real_labels = Variable(smoothed_real_labels).cuda()\n",
        "                fake_labels = Variable(fake_labels).cuda()\n",
        "\n",
        "                # Train the discriminator\n",
        "                self.discriminator.zero_grad()\n",
        "                outputs, activation_real = self.discriminator(right_images, right_embed)\n",
        "                real_loss = criterion(outputs, smoothed_real_labels)\n",
        "                real_score = outputs\n",
        "\n",
        "                if cls:\n",
        "                    outputs, _ = self.discriminator(wrong_images, right_embed)\n",
        "                    wrong_loss = criterion(outputs, fake_labels)\n",
        "                    wrong_score = outputs\n",
        "\n",
        "                if self.args.remove_noise:\n",
        "                    noise = Variable(torch.zeros(right_images.size(0), self.noise_dim)).cuda()\n",
        "                else:\n",
        "                    noise = Variable(torch.randn(right_images.size(0), self.noise_dim)).cuda()\n",
        "                noise = noise.view(noise.size(0), 100, 1, 1)\n",
        "                fake_images = self.generator(right_embed, noise, noise)\n",
        "                outputs, _ = self.discriminator(fake_images, right_embed)\n",
        "                fake_loss = criterion(outputs, fake_labels)\n",
        "                fake_score = outputs\n",
        "\n",
        "                d_loss = real_loss + fake_loss\n",
        "\n",
        "                if cls:\n",
        "                    d_loss = d_loss + wrong_loss\n",
        "\n",
        "                d_loss.backward()\n",
        "                self.optimD.step()\n",
        "\n",
        "                # Train the generator\n",
        "                self.generator.zero_grad()\n",
        "                if self.args.remove_noise:\n",
        "                    noise = Variable(torch.zeros(right_images.size(0), self.noise_dim)).cuda()\n",
        "                else:\n",
        "                    noise = Variable(torch.randn(right_images.size(0), self.noise_dim)).cuda()\n",
        "                noise = noise.view(noise.size(0), 100, 1, 1)\n",
        "                fake_images = self.generator(right_embed, noise, noise)\n",
        "                outputs, activation_fake = self.discriminator(fake_images, right_embed)\n",
        "                _, activation_real = self.discriminator(right_images, right_embed)\n",
        "\n",
        "                activation_fake = torch.mean(activation_fake, 0)\n",
        "                activation_real = torch.mean(activation_real, 0)\n",
        "                # ======= Generator Loss function============\n",
        "                # This is a customized loss function, the first term is the regular cross entropy loss\n",
        "                # The second term is feature matching loss, this measure the distance between the real and generated\n",
        "                # images statistics by comparing intermediate layers activations\n",
        "                # The third term is L1 distance between the generated and real images, this is helpful for the conditional case\n",
        "                # because it links the embedding feature vector directly to certain pixel values.\n",
        "                # ===========================================\n",
        "                g_loss = criterion(outputs, real_labels) +\\\n",
        "                         self.l2_coef * l2_loss(activation_fake, activation_real.detach()) +\\\n",
        "                         self.l1_coef * l1_loss(fake_images, right_images)\n",
        "\n",
        "                g_loss.backward()\n",
        "                self.optimG.step()\n",
        "\n",
        "                if iteration % 10 == 0:\n",
        "                    print(\"Epoch: %d, d_loss= %f, g_loss= %f, ccaD(X)= %f, D(G(X))= %f\" % (epoch, d_loss.data.cpu().mean(), g_loss.data.cpu().mean(), real_score.data.cpu().mean(), fake_score.data.cpu().mean()))\n",
        "\n",
        "            if (epoch) % 10 == 0:\n",
        "                Utils.save_checkpoint(self.discriminator, self.generator, self.save_path, epoch)\n",
        "\n",
        "    def test(self):\n",
        "        self.generator.eval()\n",
        "        self.target_generator.eval()\n",
        "        number_of_images = 2\n",
        "        sample = next(iter(self.data_loader))\n",
        "        all_nn_texts = []\n",
        "        all_nn_images = []\n",
        "        all_fake_sources = []\n",
        "        all_transfers = []\n",
        "        text = sample['txt']\n",
        "        right_images = sample['right_images']\n",
        "        right_embed = sample['right_embed']\n",
        "        for i in range(number_of_images):\n",
        "            right_images_v = Variable(right_images.float(), volatile=True)\n",
        "            right_embed_v = Variable(right_embed.float(), volatile=True)\n",
        "            if self.args.remove_noise:\n",
        "                noise = Variable(torch.zeros(right_images_v.size(0), self.noise_dim), volatile=True)\n",
        "            else:\n",
        "                noise = Variable(torch.randn(right_images_v.size(0), self.noise_dim), volatile=True)\n",
        "            if self.cuda:\n",
        "                right_embed_v = right_embed_v.cuda()\n",
        "                noise = noise.cuda()\n",
        "\n",
        "            noise = noise.view(noise.size(0), self.noise_dim, 1, 1)\n",
        "            #print(\"right_embed_v type = \",type(right_embed_v),\"noise = \",type(noise))\n",
        "            fake_target = self.target_generator.generator_only(self.generator.encoder_only(right_embed_v, noise, noise))\n",
        "            all_transfers.append(fake_target)\n",
        "            fake_source = self.generator(right_embed_v, noise,noise)\n",
        "            all_fake_sources.append(fake_source)\n",
        "\n",
        "            fake_source = fake_source.cuda()\n",
        "            print(\"fake_source shape: \",fake_source.detach().shape)\n",
        "            print(\"text description: \",text[0])\n",
        "            print(\"fake_source[0]: \",fake_source[0])\n",
        "            plt.imshow(fake_source[0].cpu().detach().permute(1, 2, 0))\n",
        "            plt.show()\n",
        "\n",
        "            nn_text, nn_images = self.nn.get_text_and_images(fake_target, -1)\n",
        "            all_nn_texts.append(nn_text)\n",
        "            all_nn_images.append(nn_images)\n",
        "\n",
        "        for i, sentence in enumerate(text):\n",
        "            nn_sentences = [sents[i] for sents in all_nn_texts]\n",
        "            print(\"\\ncombined text: \",i,\"original sentence: \",sentence,\"nn_sentences: \",nn_sentences)\n",
        "\n",
        "        for i, image in enumerate(right_images):\n",
        "            nn_images = [imgs[i] for imgs in all_nn_images]\n",
        "            fake_source_images = [imgs[i].data.cpu().numpy() for imgs in all_fake_sources]\n",
        "            transfers_images = [imgs[i].data.cpu().numpy() for imgs in all_transfers]\n",
        "            image_tile = np.tile(image, (len(nn_images), 1, 1, 1))\n",
        "            print(image_tile)\n",
        "            plt.imshow(fake_source_images)\n",
        "            plt.show()\n",
        "            plt.imshow(transfers_images)\n",
        "            plt.show()\n",
        "            #self.logger.draw_test(image_tile, fake_source_images, transfers_images, nn_images, 'image {}'.format(i))\n",
        "        print(\"*** end of testing ***\")\n",
        "\n",
        "################ trainer.py ends here ###################\n",
        "\n"
      ],
      "metadata": {
        "id": "MYWIbmsJuF-s"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################ runtime.py ###################\n",
        "class Struct:\n",
        "    def __init__(self, **entries):\n",
        "        self.__dict__.update(entries)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "params = dict()\n",
        "\n",
        "params['type']='gan' #change this if you want to train any other gan\n",
        "params['target_type']='vae'\n",
        "params['lr']=0.0002\n",
        "params['l1_coef']=50\n",
        "params['l2_coef']=100\n",
        "params['diter']=5\n",
        "params['cls']=False\n",
        "params['save_path']='tmp/'\n",
        "params['inference']=False\n",
        "params['target_train']=False\n",
        "params['dataset']='flowers_only'\n",
        "params['split']=0\n",
        "params['batch_size']=128\n",
        "params['num_workers']=1\n",
        "params['ngf']=64\n",
        "params['epochs']=20\n",
        "params['remove_noise']=False\n",
        "params['remove_noise_2']=False\n",
        "params['variational']=False\n",
        "params['vis_screen']=False\n",
        "params['pre_trained_disc']=False\n",
        "params['pre_trained_gen']=False\n",
        "# params['flowers_dataset_path']=\"../input/flowershd5dataset/flowers-hd5/data/flowers/flowers.hdf5\"\n",
        "params['flowers_dataset_path']=hdf5_fpath\n",
        "\n",
        "args = Struct(**params) #Convert nested Python dict to object\n",
        "\n",
        "trainer = Trainer(type=args.type, dataset=args.dataset, split=args.split, lr=args.lr, diter=args.diter,\n",
        "                  vis_screen=args.vis_screen, save_path=args.save_path, l1_coef=args.l1_coef,\n",
        "                  l2_coef=args.l2_coef,pre_trained_disc=args.pre_trained_disc,\n",
        "                  pre_trained_gen=args.pre_trained_gen, batch_size=args.batch_size,\n",
        "                  num_workers=args.num_workers, epochs=args.epochs, args=args)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "if not args.inference:\n",
        "    if args.target_train:\n",
        "        trainer.target_train(args.cls)\n",
        "    else:\n",
        "        trainer.train(args.cls)\n",
        "print(\"*** Calling test() ***\")\n",
        "trainer.test()\n",
        "\n",
        "elapsed = str(timedelta(seconds=int(time.time() - start_time)))\n",
        "print('Running {} took {}'.format(\"GAN-CLS\", elapsed))\n",
        "\n",
        "################ runtime.py ends here ###################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oeAAQXBQuSnW",
        "outputId": "a47655a0-8bf6-4c03-8c4d-63cfcdbbaaa9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating fresh params for discriminator\n",
            "creating fresh params for generator\n",
            "***Calling Nearest Neighbour***\n",
            "data_path:  source_flowers_only_nn_data.pl\n",
            "data_path: source_flowers_only_nn_labels.pl\n",
            "data_path: source_flowers_only_nn.pl\n",
            "data_path: source_flowers_only_nn_embeddings.pl\n",
            "start creating data for NN test source_flowers_only_nn_data.pl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-590872e199ff>:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  data_var = Variable(sample['right_images'].float(), volatile=True)\n",
            "<ipython-input-8-590872e199ff>:42: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  data_var = Variable(sample['right_images'].float(), volatile=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finish loading data for NN test\n",
            "*** Inside train() func ***\n",
            "*** Inside _train_gan() func ***\n",
            "Epoch: 0, d_loss= 2.359625, g_loss= 40.215302, ccaD(X)= 0.779924, D(G(X))= 0.733503\n",
            "Epoch: 0, d_loss= 2.141276, g_loss= 37.326611, ccaD(X)= 0.713258, D(G(X))= 0.695277\n",
            "Epoch: 0, d_loss= 1.225104, g_loss= 28.606930, ccaD(X)= 0.451852, D(G(X))= 0.105568\n",
            "Epoch: 0, d_loss= 1.503814, g_loss= 31.895203, ccaD(X)= 0.818662, D(G(X))= 0.476007\n",
            "Epoch: 0, d_loss= 1.347571, g_loss= 27.298159, ccaD(X)= 0.336613, D(G(X))= 0.025685\n",
            "Epoch: 0, d_loss= 0.765134, g_loss= 25.792364, ccaD(X)= 0.635526, D(G(X))= 0.113109\n",
            "Epoch: 0, d_loss= 0.761985, g_loss= 26.844431, ccaD(X)= 0.669454, D(G(X))= 0.155025\n",
            "Epoch: 0, d_loss= 1.011475, g_loss= 26.083965, ccaD(X)= 0.798178, D(G(X))= 0.390475\n",
            "Epoch: 0, d_loss= 0.998766, g_loss= 24.925343, ccaD(X)= 0.495043, D(G(X))= 0.110618\n",
            "Epoch: 0, d_loss= 1.114185, g_loss= 27.975868, ccaD(X)= 0.861273, D(G(X))= 0.487610\n",
            "Epoch: 0, d_loss= 1.200560, g_loss= 23.973112, ccaD(X)= 0.408242, D(G(X))= 0.153540\n",
            "Epoch: 0, d_loss= 1.316343, g_loss= 23.686794, ccaD(X)= 0.357863, D(G(X))= 0.088278\n",
            "Epoch: 0, d_loss= 1.480329, g_loss= 22.992630, ccaD(X)= 0.320726, D(G(X))= 0.068735\n",
            "Epoch: 0, d_loss= 1.113839, g_loss= 26.430252, ccaD(X)= 0.820689, D(G(X))= 0.477898\n",
            "Epoch: 0, d_loss= 1.106649, g_loss= 27.698080, ccaD(X)= 0.824592, D(G(X))= 0.483055\n",
            "Epoch: 0, d_loss= 1.872333, g_loss= 24.820309, ccaD(X)= 0.217520, D(G(X))= 0.096774\n",
            "Epoch: 0, d_loss= 0.873685, g_loss= 24.109646, ccaD(X)= 0.615332, D(G(X))= 0.211622\n",
            "Epoch: 0, d_loss= 0.905833, g_loss= 25.544945, ccaD(X)= 0.683498, D(G(X))= 0.273654\n",
            "Epoch: 0, d_loss= 0.743102, g_loss= 25.529915, ccaD(X)= 0.791091, D(G(X))= 0.257933\n",
            "Epoch: 0, d_loss= 0.786490, g_loss= 24.384655, ccaD(X)= 0.627387, D(G(X))= 0.173589\n",
            "Epoch: 0, d_loss= 1.237905, g_loss= 23.789722, ccaD(X)= 0.410292, D(G(X))= 0.202650\n",
            "Epoch: 0, d_loss= 1.177100, g_loss= 23.798059, ccaD(X)= 0.453692, D(G(X))= 0.213976\n",
            "Epoch: 0, d_loss= 1.120289, g_loss= 24.882977, ccaD(X)= 0.698608, D(G(X))= 0.432497\n",
            "Epoch: 1, d_loss= 1.430585, g_loss= 25.981125, ccaD(X)= 0.765490, D(G(X))= 0.599529\n",
            "Epoch: 1, d_loss= 1.064782, g_loss= 24.509832, ccaD(X)= 0.755908, D(G(X))= 0.412187\n",
            "Epoch: 1, d_loss= 1.018537, g_loss= 23.980963, ccaD(X)= 0.546123, D(G(X))= 0.257824\n",
            "Epoch: 1, d_loss= 1.080697, g_loss= 25.632269, ccaD(X)= 0.698106, D(G(X))= 0.416986\n",
            "Epoch: 1, d_loss= 1.331139, g_loss= 25.826218, ccaD(X)= 0.786924, D(G(X))= 0.559291\n",
            "Epoch: 1, d_loss= 1.494815, g_loss= 23.492771, ccaD(X)= 0.293871, D(G(X))= 0.088955\n",
            "Epoch: 1, d_loss= 1.073940, g_loss= 25.448639, ccaD(X)= 0.648065, D(G(X))= 0.385958\n",
            "Epoch: 1, d_loss= 1.030715, g_loss= 26.463318, ccaD(X)= 0.772805, D(G(X))= 0.412452\n",
            "Epoch: 1, d_loss= 1.204629, g_loss= 24.758099, ccaD(X)= 0.376609, D(G(X))= 0.103388\n",
            "Epoch: 1, d_loss= 1.259927, g_loss= 27.602406, ccaD(X)= 0.840360, D(G(X))= 0.551623\n",
            "Epoch: 1, d_loss= 1.166297, g_loss= 24.055590, ccaD(X)= 0.389672, D(G(X))= 0.126700\n",
            "Epoch: 1, d_loss= 1.303588, g_loss= 24.778742, ccaD(X)= 0.510395, D(G(X))= 0.395375\n",
            "Epoch: 1, d_loss= 0.987817, g_loss= 25.125172, ccaD(X)= 0.759384, D(G(X))= 0.403778\n",
            "Epoch: 1, d_loss= 0.949652, g_loss= 25.828739, ccaD(X)= 0.672851, D(G(X))= 0.327613\n",
            "Epoch: 1, d_loss= 1.006743, g_loss= 26.228359, ccaD(X)= 0.760753, D(G(X))= 0.415634\n",
            "Epoch: 1, d_loss= 0.896162, g_loss= 24.768654, ccaD(X)= 0.544017, D(G(X))= 0.166006\n",
            "Epoch: 1, d_loss= 1.377544, g_loss= 23.619686, ccaD(X)= 0.563884, D(G(X))= 0.476388\n",
            "Epoch: 1, d_loss= 0.826066, g_loss= 24.497501, ccaD(X)= 0.649984, D(G(X))= 0.240961\n",
            "Epoch: 1, d_loss= 1.006766, g_loss= 25.356253, ccaD(X)= 0.704134, D(G(X))= 0.390073\n",
            "Epoch: 1, d_loss= 0.865484, g_loss= 24.049683, ccaD(X)= 0.582098, D(G(X))= 0.201110\n",
            "Epoch: 1, d_loss= 1.097820, g_loss= 24.143841, ccaD(X)= 0.420133, D(G(X))= 0.128695\n",
            "Epoch: 1, d_loss= 0.907189, g_loss= 25.232098, ccaD(X)= 0.827961, D(G(X))= 0.393393\n",
            "Epoch: 1, d_loss= 1.196569, g_loss= 24.633699, ccaD(X)= 0.481107, D(G(X))= 0.307870\n",
            "Epoch: 2, d_loss= 1.080935, g_loss= 24.749380, ccaD(X)= 0.554499, D(G(X))= 0.314395\n",
            "Epoch: 2, d_loss= 1.299413, g_loss= 23.436384, ccaD(X)= 0.371870, D(G(X))= 0.211796\n",
            "Epoch: 2, d_loss= 1.387114, g_loss= 27.318760, ccaD(X)= 0.844296, D(G(X))= 0.595672\n",
            "Epoch: 2, d_loss= 0.823234, g_loss= 25.182541, ccaD(X)= 0.599143, D(G(X))= 0.196430\n",
            "Epoch: 2, d_loss= 0.749203, g_loss= 25.595554, ccaD(X)= 0.658157, D(G(X))= 0.180960\n",
            "Epoch: 2, d_loss= 1.773299, g_loss= 23.978748, ccaD(X)= 0.221823, D(G(X))= 0.116465\n",
            "Epoch: 2, d_loss= 1.114109, g_loss= 26.150082, ccaD(X)= 0.763596, D(G(X))= 0.476980\n",
            "Epoch: 2, d_loss= 0.992245, g_loss= 24.412008, ccaD(X)= 0.495385, D(G(X))= 0.152441\n",
            "Epoch: 2, d_loss= 1.104615, g_loss= 26.054560, ccaD(X)= 0.782214, D(G(X))= 0.464609\n",
            "Epoch: 2, d_loss= 1.078031, g_loss= 25.637005, ccaD(X)= 0.786310, D(G(X))= 0.465116\n",
            "Epoch: 2, d_loss= 1.116570, g_loss= 23.998878, ccaD(X)= 0.570892, D(G(X))= 0.364383\n",
            "Epoch: 2, d_loss= 1.191950, g_loss= 24.301163, ccaD(X)= 0.475817, D(G(X))= 0.283962\n",
            "Epoch: 2, d_loss= 1.003946, g_loss= 25.693087, ccaD(X)= 0.729709, D(G(X))= 0.403515\n",
            "Epoch: 2, d_loss= 0.987897, g_loss= 23.853401, ccaD(X)= 0.515745, D(G(X))= 0.191837\n",
            "Epoch: 2, d_loss= 1.107979, g_loss= 24.638060, ccaD(X)= 0.575087, D(G(X))= 0.360075\n",
            "Epoch: 2, d_loss= 1.065362, g_loss= 24.450171, ccaD(X)= 0.742644, D(G(X))= 0.436656\n",
            "Epoch: 2, d_loss= 1.124652, g_loss= 24.874760, ccaD(X)= 0.756574, D(G(X))= 0.482314\n",
            "Epoch: 2, d_loss= 1.098150, g_loss= 24.510468, ccaD(X)= 0.540743, D(G(X))= 0.324561\n",
            "Epoch: 2, d_loss= 1.160864, g_loss= 25.636246, ccaD(X)= 0.719995, D(G(X))= 0.475820\n",
            "Epoch: 2, d_loss= 1.170107, g_loss= 26.359095, ccaD(X)= 0.861949, D(G(X))= 0.516455\n",
            "Epoch: 2, d_loss= 1.229261, g_loss= 23.345455, ccaD(X)= 0.399020, D(G(X))= 0.222991\n",
            "Epoch: 2, d_loss= 1.227611, g_loss= 26.742434, ccaD(X)= 0.828281, D(G(X))= 0.543310\n",
            "Epoch: 2, d_loss= 1.283481, g_loss= 26.746069, ccaD(X)= 0.821158, D(G(X))= 0.560426\n",
            "Epoch: 3, d_loss= 1.330339, g_loss= 25.287046, ccaD(X)= 0.813031, D(G(X))= 0.580842\n",
            "Epoch: 3, d_loss= 1.035804, g_loss= 23.960995, ccaD(X)= 0.722306, D(G(X))= 0.417827\n",
            "Epoch: 3, d_loss= 1.017657, g_loss= 24.171360, ccaD(X)= 0.457295, D(G(X))= 0.158081\n",
            "Epoch: 3, d_loss= 1.169331, g_loss= 24.702007, ccaD(X)= 0.525279, D(G(X))= 0.353448\n",
            "Epoch: 3, d_loss= 1.018278, g_loss= 24.435415, ccaD(X)= 0.680396, D(G(X))= 0.377433\n",
            "Epoch: 3, d_loss= 1.049234, g_loss= 25.038383, ccaD(X)= 0.516027, D(G(X))= 0.266736\n",
            "Epoch: 3, d_loss= 1.362217, g_loss= 24.327541, ccaD(X)= 0.348457, D(G(X))= 0.208123\n",
            "Epoch: 3, d_loss= 1.028427, g_loss= 24.678587, ccaD(X)= 0.717995, D(G(X))= 0.421574\n",
            "Epoch: 3, d_loss= 0.910153, g_loss= 24.457176, ccaD(X)= 0.715258, D(G(X))= 0.348822\n",
            "Epoch: 3, d_loss= 1.038445, g_loss= 26.164761, ccaD(X)= 0.683881, D(G(X))= 0.395817\n",
            "Epoch: 3, d_loss= 1.212890, g_loss= 23.935869, ccaD(X)= 0.501630, D(G(X))= 0.343704\n",
            "Epoch: 3, d_loss= 1.033126, g_loss= 23.894110, ccaD(X)= 0.687154, D(G(X))= 0.408229\n",
            "Epoch: 3, d_loss= 1.309513, g_loss= 25.478971, ccaD(X)= 0.631259, D(G(X))= 0.500099\n",
            "Epoch: 3, d_loss= 1.013671, g_loss= 23.702652, ccaD(X)= 0.566053, D(G(X))= 0.301650\n",
            "Epoch: 3, d_loss= 1.222413, g_loss= 25.407570, ccaD(X)= 0.790035, D(G(X))= 0.530355\n",
            "Epoch: 3, d_loss= 1.180847, g_loss= 23.851841, ccaD(X)= 0.416442, D(G(X))= 0.216267\n",
            "Epoch: 3, d_loss= 1.453735, g_loss= 23.169670, ccaD(X)= 0.333464, D(G(X))= 0.265757\n",
            "Epoch: 3, d_loss= 1.130161, g_loss= 23.567776, ccaD(X)= 0.585270, D(G(X))= 0.390898\n",
            "Epoch: 3, d_loss= 1.197391, g_loss= 23.840292, ccaD(X)= 0.360493, D(G(X))= 0.124274\n",
            "Epoch: 3, d_loss= 1.093540, g_loss= 23.800922, ccaD(X)= 0.429848, D(G(X))= 0.183249\n",
            "Epoch: 3, d_loss= 1.074850, g_loss= 24.737835, ccaD(X)= 0.728127, D(G(X))= 0.443823\n",
            "Epoch: 3, d_loss= 2.078187, g_loss= 23.155893, ccaD(X)= 0.141688, D(G(X))= 0.051640\n",
            "Epoch: 3, d_loss= 1.559459, g_loss= 26.050867, ccaD(X)= 0.705216, D(G(X))= 0.633810\n",
            "Epoch: 4, d_loss= 1.062861, g_loss= 23.564232, ccaD(X)= 0.519781, D(G(X))= 0.283924\n",
            "Epoch: 4, d_loss= 1.120375, g_loss= 25.423964, ccaD(X)= 0.795248, D(G(X))= 0.491128\n",
            "Epoch: 4, d_loss= 1.085428, g_loss= 23.643423, ccaD(X)= 0.489542, D(G(X))= 0.247228\n",
            "Epoch: 4, d_loss= 1.127451, g_loss= 25.327339, ccaD(X)= 0.638876, D(G(X))= 0.417508\n",
            "Epoch: 4, d_loss= 1.461545, g_loss= 24.172215, ccaD(X)= 0.268329, D(G(X))= 0.106698\n",
            "Epoch: 4, d_loss= 1.153331, g_loss= 25.613522, ccaD(X)= 0.824779, D(G(X))= 0.524742\n",
            "Epoch: 4, d_loss= 1.217449, g_loss= 23.731628, ccaD(X)= 0.391286, D(G(X))= 0.203009\n",
            "Epoch: 4, d_loss= 1.010778, g_loss= 24.029667, ccaD(X)= 0.534784, D(G(X))= 0.265395\n",
            "Epoch: 4, d_loss= 1.326886, g_loss= 22.505129, ccaD(X)= 0.318424, D(G(X))= 0.125079\n",
            "Epoch: 4, d_loss= 1.201222, g_loss= 25.818855, ccaD(X)= 0.816686, D(G(X))= 0.536788\n",
            "Epoch: 4, d_loss= 1.105862, g_loss= 23.546961, ccaD(X)= 0.514510, D(G(X))= 0.312871\n",
            "Epoch: 4, d_loss= 1.140957, g_loss= 23.815849, ccaD(X)= 0.430129, D(G(X))= 0.201846\n",
            "Epoch: 4, d_loss= 1.205912, g_loss= 23.628046, ccaD(X)= 0.526710, D(G(X))= 0.380580\n",
            "Epoch: 4, d_loss= 1.155093, g_loss= 23.830252, ccaD(X)= 0.491378, D(G(X))= 0.302726\n",
            "Epoch: 4, d_loss= 1.259484, g_loss= 24.703642, ccaD(X)= 0.323295, D(G(X))= 0.061117\n",
            "Epoch: 4, d_loss= 1.147622, g_loss= 24.382370, ccaD(X)= 0.587784, D(G(X))= 0.399904\n",
            "Epoch: 4, d_loss= 0.867797, g_loss= 23.500654, ccaD(X)= 0.583413, D(G(X))= 0.214532\n",
            "Epoch: 4, d_loss= 1.068799, g_loss= 25.187433, ccaD(X)= 0.706282, D(G(X))= 0.430320\n",
            "Epoch: 4, d_loss= 1.561785, g_loss= 25.712015, ccaD(X)= 0.859432, D(G(X))= 0.671769\n",
            "Epoch: 4, d_loss= 1.263044, g_loss= 22.983185, ccaD(X)= 0.387219, D(G(X))= 0.242219\n",
            "Epoch: 4, d_loss= 1.402502, g_loss= 23.504490, ccaD(X)= 0.324827, D(G(X))= 0.221728\n",
            "Epoch: 4, d_loss= 1.180164, g_loss= 24.292336, ccaD(X)= 0.579682, D(G(X))= 0.406868\n",
            "Epoch: 4, d_loss= 1.327153, g_loss= 23.567247, ccaD(X)= 0.324299, D(G(X))= 0.123182\n",
            "Epoch: 5, d_loss= 1.387148, g_loss= 23.590662, ccaD(X)= 0.310381, D(G(X))= 0.168169\n",
            "Epoch: 5, d_loss= 0.903892, g_loss= 24.272676, ccaD(X)= 0.671789, D(G(X))= 0.318136\n",
            "Epoch: 5, d_loss= 1.078886, g_loss= 23.994331, ccaD(X)= 0.528750, D(G(X))= 0.309583\n",
            "Epoch: 5, d_loss= 1.220082, g_loss= 23.813547, ccaD(X)= 0.488144, D(G(X))= 0.349546\n",
            "Epoch: 5, d_loss= 1.039881, g_loss= 23.254957, ccaD(X)= 0.451494, D(G(X))= 0.176750\n",
            "Epoch: 5, d_loss= 0.957205, g_loss= 24.862003, ccaD(X)= 0.687665, D(G(X))= 0.361288\n",
            "Epoch: 5, d_loss= 1.098879, g_loss= 23.739941, ccaD(X)= 0.436640, D(G(X))= 0.180120\n",
            "Epoch: 5, d_loss= 0.992187, g_loss= 24.613516, ccaD(X)= 0.634034, D(G(X))= 0.345936\n",
            "Epoch: 5, d_loss= 1.237746, g_loss= 24.426025, ccaD(X)= 0.718195, D(G(X))= 0.511803\n",
            "Epoch: 5, d_loss= 1.117679, g_loss= 23.880367, ccaD(X)= 0.498281, D(G(X))= 0.291664\n",
            "Epoch: 5, d_loss= 1.094399, g_loss= 23.190628, ccaD(X)= 0.491099, D(G(X))= 0.275612\n",
            "Epoch: 5, d_loss= 0.822255, g_loss= 24.690907, ccaD(X)= 0.673631, D(G(X))= 0.263907\n",
            "Epoch: 5, d_loss= 0.957349, g_loss= 25.664391, ccaD(X)= 0.751192, D(G(X))= 0.395455\n",
            "Epoch: 5, d_loss= 0.958159, g_loss= 23.696760, ccaD(X)= 0.530786, D(G(X))= 0.226522\n",
            "Epoch: 5, d_loss= 1.149021, g_loss= 23.505001, ccaD(X)= 0.467050, D(G(X))= 0.263969\n",
            "Epoch: 5, d_loss= 0.989686, g_loss= 24.131447, ccaD(X)= 0.520546, D(G(X))= 0.240464\n",
            "Epoch: 5, d_loss= 1.129396, g_loss= 25.044489, ccaD(X)= 0.834869, D(G(X))= 0.504501\n",
            "Epoch: 5, d_loss= 1.053676, g_loss= 24.960791, ccaD(X)= 0.721418, D(G(X))= 0.437704\n",
            "Epoch: 5, d_loss= 0.962945, g_loss= 25.038267, ccaD(X)= 0.675814, D(G(X))= 0.359331\n",
            "Epoch: 5, d_loss= 1.209556, g_loss= 23.934242, ccaD(X)= 0.747769, D(G(X))= 0.514997\n",
            "Epoch: 5, d_loss= 1.319088, g_loss= 25.645769, ccaD(X)= 0.793925, D(G(X))= 0.569203\n",
            "Epoch: 5, d_loss= 0.961253, g_loss= 24.000301, ccaD(X)= 0.542419, D(G(X))= 0.239052\n",
            "Epoch: 5, d_loss= 0.789857, g_loss= 25.046017, ccaD(X)= 0.620805, D(G(X))= 0.193885\n",
            "Epoch: 6, d_loss= 0.793624, g_loss= 24.104824, ccaD(X)= 0.639791, D(G(X))= 0.221785\n",
            "Epoch: 6, d_loss= 1.054506, g_loss= 23.828957, ccaD(X)= 0.551396, D(G(X))= 0.309997\n",
            "Epoch: 6, d_loss= 1.136913, g_loss= 23.664810, ccaD(X)= 0.529701, D(G(X))= 0.353383\n",
            "Epoch: 6, d_loss= 1.050376, g_loss= 23.163736, ccaD(X)= 0.425293, D(G(X))= 0.148312\n",
            "Epoch: 6, d_loss= 1.352405, g_loss= 23.377045, ccaD(X)= 0.441667, D(G(X))= 0.384365\n",
            "Epoch: 6, d_loss= 1.198735, g_loss= 23.817291, ccaD(X)= 0.367048, D(G(X))= 0.159094\n",
            "Epoch: 6, d_loss= 1.234102, g_loss= 24.538942, ccaD(X)= 0.718189, D(G(X))= 0.504955\n",
            "Epoch: 6, d_loss= 1.113349, g_loss= 23.450068, ccaD(X)= 0.446039, D(G(X))= 0.234944\n",
            "Epoch: 6, d_loss= 1.004519, g_loss= 23.834793, ccaD(X)= 0.652965, D(G(X))= 0.372342\n",
            "Epoch: 6, d_loss= 2.035719, g_loss= 23.517477, ccaD(X)= 0.141857, D(G(X))= 0.108673\n",
            "Epoch: 6, d_loss= 1.015689, g_loss= 24.970179, ccaD(X)= 0.704067, D(G(X))= 0.406862\n",
            "Epoch: 6, d_loss= 1.095611, g_loss= 24.147175, ccaD(X)= 0.440874, D(G(X))= 0.180826\n",
            "Epoch: 6, d_loss= 1.263730, g_loss= 23.180815, ccaD(X)= 0.450929, D(G(X))= 0.338825\n",
            "Epoch: 6, d_loss= 1.204816, g_loss= 23.974148, ccaD(X)= 0.743029, D(G(X))= 0.516515\n",
            "Epoch: 6, d_loss= 1.144154, g_loss= 24.368208, ccaD(X)= 0.529649, D(G(X))= 0.307497\n",
            "Epoch: 6, d_loss= 1.152648, g_loss= 24.055532, ccaD(X)= 0.673783, D(G(X))= 0.454677\n",
            "Epoch: 6, d_loss= 0.917948, g_loss= 23.926849, ccaD(X)= 0.548000, D(G(X))= 0.218841\n",
            "Epoch: 6, d_loss= 1.653992, g_loss= 23.210178, ccaD(X)= 0.201265, D(G(X))= 0.034768\n",
            "Epoch: 6, d_loss= 1.023170, g_loss= 24.689785, ccaD(X)= 0.710188, D(G(X))= 0.416196\n",
            "Epoch: 6, d_loss= 1.189248, g_loss= 24.687462, ccaD(X)= 0.602568, D(G(X))= 0.435039\n",
            "Epoch: 6, d_loss= 0.993965, g_loss= 24.212959, ccaD(X)= 0.719128, D(G(X))= 0.403499\n",
            "Epoch: 6, d_loss= 1.059368, g_loss= 24.228416, ccaD(X)= 0.666187, D(G(X))= 0.410090\n",
            "Epoch: 6, d_loss= 0.957660, g_loss= 25.158817, ccaD(X)= 0.782336, D(G(X))= 0.419270\n",
            "Epoch: 7, d_loss= 1.111154, g_loss= 23.652191, ccaD(X)= 0.576546, D(G(X))= 0.370288\n",
            "Epoch: 7, d_loss= 0.964047, g_loss= 24.764376, ccaD(X)= 0.614581, D(G(X))= 0.314759\n",
            "Epoch: 7, d_loss= 1.095111, g_loss= 22.544361, ccaD(X)= 0.409805, D(G(X))= 0.156337\n",
            "Epoch: 7, d_loss= 1.021886, g_loss= 24.318878, ccaD(X)= 0.552863, D(G(X))= 0.292834\n",
            "Epoch: 7, d_loss= 0.936994, g_loss= 25.003130, ccaD(X)= 0.820202, D(G(X))= 0.406320\n",
            "Epoch: 7, d_loss= 1.027368, g_loss= 25.302885, ccaD(X)= 0.807490, D(G(X))= 0.459270\n",
            "Epoch: 7, d_loss= 1.108005, g_loss= 23.030548, ccaD(X)= 0.458058, D(G(X))= 0.230059\n",
            "Epoch: 7, d_loss= 1.000612, g_loss= 24.731255, ccaD(X)= 0.694424, D(G(X))= 0.387366\n",
            "Epoch: 7, d_loss= 0.940781, g_loss= 24.836874, ccaD(X)= 0.725405, D(G(X))= 0.371485\n",
            "Epoch: 7, d_loss= 1.066523, g_loss= 24.738367, ccaD(X)= 0.769629, D(G(X))= 0.463403\n",
            "Epoch: 7, d_loss= 0.982286, g_loss= 23.217789, ccaD(X)= 0.545214, D(G(X))= 0.252803\n",
            "Epoch: 7, d_loss= 1.340600, g_loss= 24.842209, ccaD(X)= 0.834016, D(G(X))= 0.597661\n",
            "Epoch: 7, d_loss= 1.365447, g_loss= 23.862274, ccaD(X)= 0.299152, D(G(X))= 0.140350\n",
            "Epoch: 7, d_loss= 1.174739, g_loss= 23.537224, ccaD(X)= 0.362690, D(G(X))= 0.124611\n",
            "Epoch: 7, d_loss= 1.020170, g_loss= 24.398523, ccaD(X)= 0.705877, D(G(X))= 0.413121\n",
            "Epoch: 7, d_loss= 1.426193, g_loss= 23.163502, ccaD(X)= 0.321091, D(G(X))= 0.225478\n",
            "Epoch: 7, d_loss= 0.978552, g_loss= 24.317192, ccaD(X)= 0.630798, D(G(X))= 0.336375\n",
            "Epoch: 7, d_loss= 1.252080, g_loss= 24.509775, ccaD(X)= 0.701822, D(G(X))= 0.513479\n",
            "Epoch: 7, d_loss= 0.954203, g_loss= 25.551432, ccaD(X)= 0.801787, D(G(X))= 0.416287\n",
            "Epoch: 7, d_loss= 1.166017, g_loss= 23.584118, ccaD(X)= 0.530751, D(G(X))= 0.364215\n",
            "Epoch: 7, d_loss= 1.240210, g_loss= 25.279299, ccaD(X)= 0.750364, D(G(X))= 0.535663\n",
            "Epoch: 7, d_loss= 0.917368, g_loss= 24.124182, ccaD(X)= 0.682276, D(G(X))= 0.337806\n",
            "Epoch: 7, d_loss= 0.840805, g_loss= 24.762320, ccaD(X)= 0.749380, D(G(X))= 0.323965\n",
            "Epoch: 8, d_loss= 1.334357, g_loss= 24.991993, ccaD(X)= 0.780374, D(G(X))= 0.580148\n",
            "Epoch: 8, d_loss= 0.874424, g_loss= 23.558842, ccaD(X)= 0.602779, D(G(X))= 0.246037\n",
            "Epoch: 8, d_loss= 1.171196, g_loss= 23.377295, ccaD(X)= 0.362381, D(G(X))= 0.110379\n",
            "Epoch: 8, d_loss= 0.709036, g_loss= 24.818823, ccaD(X)= 0.678286, D(G(X))= 0.192109\n",
            "Epoch: 8, d_loss= 1.069439, g_loss= 24.107088, ccaD(X)= 0.666368, D(G(X))= 0.410658\n",
            "Epoch: 8, d_loss= 1.055448, g_loss= 23.673378, ccaD(X)= 0.499660, D(G(X))= 0.248946\n",
            "Epoch: 8, d_loss= 1.038543, g_loss= 24.160431, ccaD(X)= 0.611605, D(G(X))= 0.359849\n",
            "Epoch: 8, d_loss= 1.514152, g_loss= 23.975475, ccaD(X)= 0.242027, D(G(X))= 0.095123\n",
            "Epoch: 8, d_loss= 1.000209, g_loss= 23.665039, ccaD(X)= 0.619969, D(G(X))= 0.342682\n",
            "Epoch: 8, d_loss= 0.893116, g_loss= 23.839724, ccaD(X)= 0.687512, D(G(X))= 0.318685\n",
            "Epoch: 8, d_loss= 1.364073, g_loss= 23.657179, ccaD(X)= 0.283671, D(G(X))= 0.048574\n",
            "Epoch: 8, d_loss= 1.064303, g_loss= 24.325136, ccaD(X)= 0.490477, D(G(X))= 0.241176\n",
            "Epoch: 8, d_loss= 0.828900, g_loss= 24.109165, ccaD(X)= 0.642440, D(G(X))= 0.248575\n",
            "Epoch: 8, d_loss= 0.967159, g_loss= 24.091578, ccaD(X)= 0.595527, D(G(X))= 0.295595\n",
            "Epoch: 8, d_loss= 1.146392, g_loss= 25.346346, ccaD(X)= 0.758362, D(G(X))= 0.495834\n",
            "Epoch: 8, d_loss= 0.838082, g_loss= 24.178593, ccaD(X)= 0.590038, D(G(X))= 0.207519\n",
            "Epoch: 8, d_loss= 1.423650, g_loss= 22.346912, ccaD(X)= 0.274352, D(G(X))= 0.075896\n",
            "Epoch: 8, d_loss= 1.004355, g_loss= 23.793615, ccaD(X)= 0.553494, D(G(X))= 0.287629\n",
            "Epoch: 8, d_loss= 1.314855, g_loss= 23.442867, ccaD(X)= 0.313116, D(G(X))= 0.133978\n",
            "Epoch: 8, d_loss= 1.004760, g_loss= 23.552885, ccaD(X)= 0.427181, D(G(X))= 0.098182\n",
            "Epoch: 8, d_loss= 0.904810, g_loss= 23.712469, ccaD(X)= 0.740414, D(G(X))= 0.359312\n",
            "Epoch: 8, d_loss= 0.975937, g_loss= 23.690557, ccaD(X)= 0.663238, D(G(X))= 0.361648\n",
            "Epoch: 8, d_loss= 0.863083, g_loss= 24.043646, ccaD(X)= 0.733280, D(G(X))= 0.333705\n",
            "Epoch: 9, d_loss= 0.922333, g_loss= 24.388741, ccaD(X)= 0.682645, D(G(X))= 0.336602\n",
            "Epoch: 9, d_loss= 1.181139, g_loss= 23.305807, ccaD(X)= 0.369477, D(G(X))= 0.140410\n",
            "Epoch: 9, d_loss= 0.784416, g_loss= 25.168550, ccaD(X)= 0.724118, D(G(X))= 0.279965\n",
            "Epoch: 9, d_loss= 1.235987, g_loss= 22.955799, ccaD(X)= 0.352202, D(G(X))= 0.111823\n",
            "Epoch: 9, d_loss= 0.837462, g_loss= 23.813538, ccaD(X)= 0.594312, D(G(X))= 0.211562\n",
            "Epoch: 9, d_loss= 0.793691, g_loss= 24.851711, ccaD(X)= 0.673699, D(G(X))= 0.246637\n",
            "Epoch: 9, d_loss= 1.035973, g_loss= 23.547825, ccaD(X)= 0.599423, D(G(X))= 0.346369\n",
            "Epoch: 9, d_loss= 0.992517, g_loss= 23.696190, ccaD(X)= 0.540502, D(G(X))= 0.254151\n",
            "Epoch: 9, d_loss= 0.972441, g_loss= 24.208086, ccaD(X)= 0.757547, D(G(X))= 0.410404\n",
            "Epoch: 9, d_loss= 0.783868, g_loss= 24.483625, ccaD(X)= 0.666191, D(G(X))= 0.226651\n",
            "Epoch: 9, d_loss= 0.989389, g_loss= 24.737049, ccaD(X)= 0.737784, D(G(X))= 0.410976\n",
            "Epoch: 9, d_loss= 0.900941, g_loss= 24.282631, ccaD(X)= 0.817789, D(G(X))= 0.394804\n",
            "Epoch: 9, d_loss= 0.981536, g_loss= 24.014696, ccaD(X)= 0.544644, D(G(X))= 0.257259\n",
            "Epoch: 9, d_loss= 1.264555, g_loss= 26.370232, ccaD(X)= 0.857771, D(G(X))= 0.569020\n",
            "Epoch: 9, d_loss= 1.173215, g_loss= 23.708841, ccaD(X)= 0.398677, D(G(X))= 0.182011\n",
            "Epoch: 9, d_loss= 0.793673, g_loss= 23.768221, ccaD(X)= 0.582897, D(G(X))= 0.166728\n",
            "Epoch: 9, d_loss= 1.884543, g_loss= 22.277937, ccaD(X)= 0.161990, D(G(X))= 0.083632\n",
            "Epoch: 9, d_loss= 1.083991, g_loss= 24.010990, ccaD(X)= 0.743717, D(G(X))= 0.458233\n",
            "Epoch: 9, d_loss= 0.841709, g_loss= 23.498301, ccaD(X)= 0.556492, D(G(X))= 0.172356\n",
            "Epoch: 9, d_loss= 1.170734, g_loss= 24.088432, ccaD(X)= 0.346821, D(G(X))= 0.081902\n",
            "Epoch: 9, d_loss= 1.090299, g_loss= 23.357321, ccaD(X)= 0.397574, D(G(X))= 0.129022\n",
            "Epoch: 9, d_loss= 0.848058, g_loss= 23.735220, ccaD(X)= 0.619591, D(G(X))= 0.244820\n",
            "Epoch: 9, d_loss= 0.749786, g_loss= 24.267570, ccaD(X)= 0.594675, D(G(X))= 0.150551\n",
            "Epoch: 10, d_loss= 0.955795, g_loss= 24.292385, ccaD(X)= 0.649929, D(G(X))= 0.334009\n",
            "Epoch: 10, d_loss= 0.890816, g_loss= 24.343748, ccaD(X)= 0.490809, D(G(X))= 0.109830\n",
            "Epoch: 10, d_loss= 0.889917, g_loss= 25.043932, ccaD(X)= 0.830353, D(G(X))= 0.389258\n",
            "Epoch: 10, d_loss= 0.890646, g_loss= 24.377571, ccaD(X)= 0.709918, D(G(X))= 0.339231\n",
            "Epoch: 10, d_loss= 1.237072, g_loss= 22.899530, ccaD(X)= 0.334659, D(G(X))= 0.107109\n",
            "Epoch: 10, d_loss= 0.823002, g_loss= 25.212612, ccaD(X)= 0.784911, D(G(X))= 0.340122\n",
            "Epoch: 10, d_loss= 0.934427, g_loss= 26.150734, ccaD(X)= 0.760046, D(G(X))= 0.388810\n",
            "Epoch: 10, d_loss= 0.914731, g_loss= 25.017357, ccaD(X)= 0.812867, D(G(X))= 0.395686\n",
            "Epoch: 10, d_loss= 1.211900, g_loss= 23.219946, ccaD(X)= 0.329306, D(G(X))= 0.076898\n",
            "Epoch: 10, d_loss= 0.930719, g_loss= 24.919876, ccaD(X)= 0.775331, D(G(X))= 0.392698\n",
            "Epoch: 10, d_loss= 0.907071, g_loss= 24.880074, ccaD(X)= 0.737195, D(G(X))= 0.363283\n",
            "Epoch: 10, d_loss= 0.843905, g_loss= 25.674906, ccaD(X)= 0.757339, D(G(X))= 0.338141\n",
            "Epoch: 10, d_loss= 1.134799, g_loss= 23.577089, ccaD(X)= 0.352865, D(G(X))= 0.070988\n",
            "Epoch: 10, d_loss= 0.996985, g_loss= 22.995546, ccaD(X)= 0.465217, D(G(X))= 0.148706\n",
            "Epoch: 10, d_loss= 0.964899, g_loss= 24.744871, ccaD(X)= 0.791308, D(G(X))= 0.409053\n",
            "Epoch: 10, d_loss= 1.344317, g_loss= 22.734951, ccaD(X)= 0.304416, D(G(X))= 0.067016\n",
            "Epoch: 10, d_loss= 0.854245, g_loss= 24.686049, ccaD(X)= 0.580543, D(G(X))= 0.200394\n",
            "Epoch: 10, d_loss= 0.868347, g_loss= 24.286400, ccaD(X)= 0.619777, D(G(X))= 0.252166\n",
            "Epoch: 10, d_loss= 1.443506, g_loss= 23.425810, ccaD(X)= 0.250540, D(G(X))= 0.063863\n",
            "Epoch: 10, d_loss= 1.009548, g_loss= 24.003899, ccaD(X)= 0.820173, D(G(X))= 0.447334\n",
            "Epoch: 10, d_loss= 0.886933, g_loss= 24.075897, ccaD(X)= 0.683019, D(G(X))= 0.315418\n",
            "Epoch: 10, d_loss= 0.904265, g_loss= 24.125313, ccaD(X)= 0.603784, D(G(X))= 0.264471\n",
            "Epoch: 10, d_loss= 0.923738, g_loss= 24.681149, ccaD(X)= 0.699683, D(G(X))= 0.354216\n",
            "Epoch: 11, d_loss= 1.028760, g_loss= 25.779875, ccaD(X)= 0.706611, D(G(X))= 0.415484\n",
            "Epoch: 11, d_loss= 0.921675, g_loss= 23.918047, ccaD(X)= 0.599030, D(G(X))= 0.271111\n",
            "Epoch: 11, d_loss= 1.081438, g_loss= 25.056347, ccaD(X)= 0.839901, D(G(X))= 0.481360\n",
            "Epoch: 11, d_loss= 0.819795, g_loss= 25.038206, ccaD(X)= 0.610784, D(G(X))= 0.209446\n",
            "Epoch: 11, d_loss= 0.870122, g_loss= 23.278360, ccaD(X)= 0.700794, D(G(X))= 0.310143\n",
            "Epoch: 11, d_loss= 0.898768, g_loss= 23.619705, ccaD(X)= 0.664507, D(G(X))= 0.316521\n",
            "Epoch: 11, d_loss= 0.914771, g_loss= 24.820316, ccaD(X)= 0.792631, D(G(X))= 0.394251\n",
            "Epoch: 11, d_loss= 0.909969, g_loss= 25.145386, ccaD(X)= 0.705351, D(G(X))= 0.344456\n",
            "Epoch: 11, d_loss= 0.827785, g_loss= 24.727716, ccaD(X)= 0.623974, D(G(X))= 0.222304\n",
            "Epoch: 11, d_loss= 1.149109, g_loss= 26.240223, ccaD(X)= 0.881711, D(G(X))= 0.523159\n",
            "Epoch: 11, d_loss= 0.851873, g_loss= 23.750109, ccaD(X)= 0.534945, D(G(X))= 0.152487\n",
            "Epoch: 11, d_loss= 0.849861, g_loss= 24.118618, ccaD(X)= 0.619616, D(G(X))= 0.234698\n",
            "Epoch: 11, d_loss= 0.847760, g_loss= 23.492682, ccaD(X)= 0.505340, D(G(X))= 0.107776\n",
            "Epoch: 11, d_loss= 1.162436, g_loss= 25.274307, ccaD(X)= 0.747926, D(G(X))= 0.470851\n",
            "Epoch: 11, d_loss= 1.262393, g_loss= 22.988115, ccaD(X)= 0.318265, D(G(X))= 0.084316\n",
            "Epoch: 11, d_loss= 0.826341, g_loss= 24.872030, ccaD(X)= 0.776375, D(G(X))= 0.332686\n",
            "Epoch: 11, d_loss= 0.919448, g_loss= 24.231161, ccaD(X)= 0.507050, D(G(X))= 0.156700\n",
            "Epoch: 11, d_loss= 0.781291, g_loss= 24.547443, ccaD(X)= 0.597169, D(G(X))= 0.158563\n",
            "Epoch: 11, d_loss= 0.813627, g_loss= 23.573921, ccaD(X)= 0.573725, D(G(X))= 0.167969\n",
            "Epoch: 11, d_loss= 1.020934, g_loss= 23.539192, ccaD(X)= 0.419719, D(G(X))= 0.101565\n",
            "Epoch: 11, d_loss= 0.681516, g_loss= 24.248226, ccaD(X)= 0.707261, D(G(X))= 0.186663\n",
            "Epoch: 11, d_loss= 0.798248, g_loss= 23.606075, ccaD(X)= 0.590728, D(G(X))= 0.167592\n",
            "Epoch: 11, d_loss= 1.267577, g_loss= 26.569366, ccaD(X)= 0.924119, D(G(X))= 0.563580\n",
            "Epoch: 12, d_loss= 1.284745, g_loss= 22.804350, ccaD(X)= 0.314287, D(G(X))= 0.070750\n",
            "Epoch: 12, d_loss= 0.804455, g_loss= 23.835531, ccaD(X)= 0.630942, D(G(X))= 0.222308\n",
            "Epoch: 12, d_loss= 1.291221, g_loss= 25.511213, ccaD(X)= 0.905821, D(G(X))= 0.580404\n",
            "Epoch: 12, d_loss= 0.660967, g_loss= 24.297684, ccaD(X)= 0.679997, D(G(X))= 0.159871\n",
            "Epoch: 12, d_loss= 0.982280, g_loss= 26.363415, ccaD(X)= 0.865927, D(G(X))= 0.444272\n",
            "Epoch: 12, d_loss= 1.364063, g_loss= 25.044340, ccaD(X)= 0.850754, D(G(X))= 0.590622\n",
            "Epoch: 12, d_loss= 0.856447, g_loss= 23.616453, ccaD(X)= 0.531806, D(G(X))= 0.142131\n",
            "Epoch: 12, d_loss= 0.912308, g_loss= 24.549351, ccaD(X)= 0.661033, D(G(X))= 0.302677\n",
            "Epoch: 12, d_loss= 0.927574, g_loss= 25.366003, ccaD(X)= 0.822464, D(G(X))= 0.405845\n",
            "Epoch: 12, d_loss= 0.787445, g_loss= 24.924311, ccaD(X)= 0.803124, D(G(X))= 0.319165\n",
            "Epoch: 12, d_loss= 0.744702, g_loss= 24.213659, ccaD(X)= 0.591822, D(G(X))= 0.119946\n",
            "Epoch: 12, d_loss= 0.940008, g_loss= 23.258801, ccaD(X)= 0.472698, D(G(X))= 0.137205\n",
            "Epoch: 12, d_loss= 0.695664, g_loss= 24.394600, ccaD(X)= 0.713107, D(G(X))= 0.202096\n",
            "Epoch: 12, d_loss= 0.774052, g_loss= 25.197680, ccaD(X)= 0.708943, D(G(X))= 0.257756\n",
            "Epoch: 12, d_loss= 1.239518, g_loss= 26.525227, ccaD(X)= 0.876988, D(G(X))= 0.549933\n",
            "Epoch: 12, d_loss= 0.930286, g_loss= 23.209255, ccaD(X)= 0.466574, D(G(X))= 0.108022\n",
            "Epoch: 12, d_loss= 0.876240, g_loss= 24.613968, ccaD(X)= 0.684374, D(G(X))= 0.309897\n",
            "Epoch: 12, d_loss= 0.678336, g_loss= 24.730247, ccaD(X)= 0.629695, D(G(X))= 0.111257\n",
            "Epoch: 12, d_loss= 0.782758, g_loss= 25.323286, ccaD(X)= 0.729612, D(G(X))= 0.275220\n",
            "Epoch: 12, d_loss= 0.884311, g_loss= 24.531988, ccaD(X)= 0.704589, D(G(X))= 0.327951\n",
            "Epoch: 12, d_loss= 1.016942, g_loss= 25.084330, ccaD(X)= 0.846655, D(G(X))= 0.454750\n",
            "Epoch: 12, d_loss= 1.043670, g_loss= 23.430340, ccaD(X)= 0.395386, D(G(X))= 0.067154\n",
            "Epoch: 12, d_loss= 0.748740, g_loss= 24.478170, ccaD(X)= 0.581481, D(G(X))= 0.116361\n",
            "Epoch: 13, d_loss= 0.740143, g_loss= 25.346069, ccaD(X)= 0.820049, D(G(X))= 0.291895\n",
            "Epoch: 13, d_loss= 0.720310, g_loss= 24.125446, ccaD(X)= 0.753374, D(G(X))= 0.246771\n",
            "Epoch: 13, d_loss= 0.707315, g_loss= 24.688248, ccaD(X)= 0.722377, D(G(X))= 0.211835\n",
            "Epoch: 13, d_loss= 0.780771, g_loss= 24.118616, ccaD(X)= 0.685026, D(G(X))= 0.249233\n",
            "Epoch: 13, d_loss= 0.965787, g_loss= 23.316559, ccaD(X)= 0.444583, D(G(X))= 0.108852\n",
            "Epoch: 13, d_loss= 0.865333, g_loss= 24.259024, ccaD(X)= 0.493791, D(G(X))= 0.092816\n",
            "Epoch: 13, d_loss= 1.341544, g_loss= 21.757347, ccaD(X)= 0.284889, D(G(X))= 0.073501\n",
            "Epoch: 13, d_loss= 0.866680, g_loss= 23.224483, ccaD(X)= 0.511413, D(G(X))= 0.113238\n",
            "Epoch: 13, d_loss= 0.980200, g_loss= 23.498690, ccaD(X)= 0.452555, D(G(X))= 0.120614\n",
            "Epoch: 13, d_loss= 0.719108, g_loss= 23.713717, ccaD(X)= 0.739115, D(G(X))= 0.235742\n",
            "Epoch: 13, d_loss= 0.810785, g_loss= 25.280752, ccaD(X)= 0.897616, D(G(X))= 0.351041\n",
            "Epoch: 13, d_loss= 0.879190, g_loss= 23.555849, ccaD(X)= 0.474648, D(G(X))= 0.076610\n",
            "Epoch: 13, d_loss= 0.848597, g_loss= 23.368286, ccaD(X)= 0.577043, D(G(X))= 0.197580\n",
            "Epoch: 13, d_loss= 0.821550, g_loss= 24.000744, ccaD(X)= 0.609461, D(G(X))= 0.205000\n",
            "Epoch: 13, d_loss= 0.696899, g_loss= 24.877529, ccaD(X)= 0.733664, D(G(X))= 0.213897\n",
            "Epoch: 13, d_loss= 0.554347, g_loss= 25.057371, ccaD(X)= 0.748195, D(G(X))= 0.109965\n",
            "Epoch: 13, d_loss= 0.938212, g_loss= 23.632154, ccaD(X)= 0.536774, D(G(X))= 0.205398\n",
            "Epoch: 13, d_loss= 0.881922, g_loss= 23.772186, ccaD(X)= 0.482915, D(G(X))= 0.071810\n",
            "Epoch: 13, d_loss= 0.796844, g_loss= 26.149868, ccaD(X)= 0.840954, D(G(X))= 0.331112\n",
            "Epoch: 13, d_loss= 0.801392, g_loss= 23.398640, ccaD(X)= 0.609021, D(G(X))= 0.191323\n",
            "Epoch: 13, d_loss= 0.832444, g_loss= 24.302151, ccaD(X)= 0.554162, D(G(X))= 0.156090\n",
            "Epoch: 13, d_loss= 1.015357, g_loss= 23.479118, ccaD(X)= 0.394629, D(G(X))= 0.056682\n",
            "Epoch: 13, d_loss= 0.973035, g_loss= 25.558905, ccaD(X)= 0.860038, D(G(X))= 0.436033\n",
            "Epoch: 14, d_loss= 0.894586, g_loss= 25.172512, ccaD(X)= 0.909782, D(G(X))= 0.398149\n",
            "Epoch: 14, d_loss= 0.707674, g_loss= 25.877432, ccaD(X)= 0.830839, D(G(X))= 0.272324\n",
            "Epoch: 14, d_loss= 0.724330, g_loss= 25.034895, ccaD(X)= 0.677572, D(G(X))= 0.194867\n",
            "Epoch: 14, d_loss= 0.686255, g_loss= 23.752344, ccaD(X)= 0.686535, D(G(X))= 0.175587\n",
            "Epoch: 14, d_loss= 0.758384, g_loss= 24.124081, ccaD(X)= 0.623962, D(G(X))= 0.170014\n",
            "Epoch: 14, d_loss= 0.665215, g_loss= 24.178909, ccaD(X)= 0.622819, D(G(X))= 0.100756\n",
            "Epoch: 14, d_loss= 0.774593, g_loss= 23.977066, ccaD(X)= 0.552612, D(G(X))= 0.086738\n",
            "Epoch: 14, d_loss= 2.285135, g_loss= 22.892513, ccaD(X)= 0.108687, D(G(X))= 0.013485\n",
            "Epoch: 14, d_loss= 0.795235, g_loss= 25.238750, ccaD(X)= 0.874013, D(G(X))= 0.337926\n",
            "Epoch: 14, d_loss= 0.851045, g_loss= 23.624086, ccaD(X)= 0.538825, D(G(X))= 0.147183\n",
            "Epoch: 14, d_loss= 0.873708, g_loss= 24.816397, ccaD(X)= 0.715374, D(G(X))= 0.324803\n",
            "Epoch: 14, d_loss= 1.136867, g_loss= 25.245342, ccaD(X)= 0.855515, D(G(X))= 0.509760\n",
            "Epoch: 14, d_loss= 0.932435, g_loss= 24.052017, ccaD(X)= 0.456505, D(G(X))= 0.091648\n",
            "Epoch: 14, d_loss= 0.778613, g_loss= 24.553213, ccaD(X)= 0.737860, D(G(X))= 0.280819\n",
            "Epoch: 14, d_loss= 0.825759, g_loss= 25.252819, ccaD(X)= 0.727872, D(G(X))= 0.303138\n",
            "Epoch: 14, d_loss= 1.305792, g_loss= 24.253199, ccaD(X)= 0.292046, D(G(X))= 0.028238\n",
            "Epoch: 14, d_loss= 0.764560, g_loss= 24.662909, ccaD(X)= 0.822038, D(G(X))= 0.309075\n",
            "Epoch: 14, d_loss= 0.877953, g_loss= 24.264551, ccaD(X)= 0.871898, D(G(X))= 0.380247\n",
            "Epoch: 14, d_loss= 0.788928, g_loss= 23.913231, ccaD(X)= 0.575025, D(G(X))= 0.140071\n",
            "Epoch: 14, d_loss= 0.793350, g_loss= 24.236984, ccaD(X)= 0.698547, D(G(X))= 0.263492\n",
            "Epoch: 14, d_loss= 0.659683, g_loss= 25.972881, ccaD(X)= 0.854210, D(G(X))= 0.247994\n",
            "Epoch: 14, d_loss= 0.654013, g_loss= 24.363651, ccaD(X)= 0.774564, D(G(X))= 0.212459\n",
            "Epoch: 14, d_loss= 0.665558, g_loss= 25.638781, ccaD(X)= 0.773436, D(G(X))= 0.222893\n",
            "Epoch: 15, d_loss= 0.966996, g_loss= 22.808187, ccaD(X)= 0.475951, D(G(X))= 0.146043\n",
            "Epoch: 15, d_loss= 0.841458, g_loss= 23.449598, ccaD(X)= 0.499051, D(G(X))= 0.080760\n",
            "Epoch: 15, d_loss= 0.814587, g_loss= 23.724562, ccaD(X)= 0.624392, D(G(X))= 0.216314\n",
            "Epoch: 15, d_loss= 0.819781, g_loss= 24.673162, ccaD(X)= 0.833444, D(G(X))= 0.338654\n",
            "Epoch: 15, d_loss= 0.609555, g_loss= 24.259718, ccaD(X)= 0.718174, D(G(X))= 0.141552\n",
            "Epoch: 15, d_loss= 0.958169, g_loss= 26.388073, ccaD(X)= 0.933505, D(G(X))= 0.426966\n",
            "Epoch: 15, d_loss= 1.279162, g_loss= 25.703854, ccaD(X)= 0.792886, D(G(X))= 0.557602\n",
            "Epoch: 15, d_loss= 0.746977, g_loss= 24.537636, ccaD(X)= 0.645815, D(G(X))= 0.178730\n",
            "Epoch: 15, d_loss= 0.936990, g_loss= 25.001534, ccaD(X)= 0.792331, D(G(X))= 0.389183\n",
            "Epoch: 15, d_loss= 0.784970, g_loss= 26.738358, ccaD(X)= 0.892745, D(G(X))= 0.332036\n",
            "Epoch: 15, d_loss= 0.702440, g_loss= 23.425076, ccaD(X)= 0.639397, D(G(X))= 0.140196\n",
            "Epoch: 15, d_loss= 0.730310, g_loss= 24.594988, ccaD(X)= 0.567906, D(G(X))= 0.079950\n",
            "Epoch: 15, d_loss= 0.833823, g_loss= 23.924299, ccaD(X)= 0.499779, D(G(X))= 0.067593\n",
            "Epoch: 15, d_loss= 0.700138, g_loss= 24.388546, ccaD(X)= 0.592278, D(G(X))= 0.092287\n",
            "Epoch: 15, d_loss= 0.852135, g_loss= 25.552584, ccaD(X)= 0.775745, D(G(X))= 0.338071\n",
            "Epoch: 15, d_loss= 0.763817, g_loss= 23.920282, ccaD(X)= 0.616228, D(G(X))= 0.160255\n",
            "Epoch: 15, d_loss= 1.009977, g_loss= 24.869946, ccaD(X)= 0.833868, D(G(X))= 0.443055\n",
            "Epoch: 15, d_loss= 0.698562, g_loss= 24.935555, ccaD(X)= 0.752852, D(G(X))= 0.228145\n",
            "Epoch: 15, d_loss= 0.899659, g_loss= 25.625563, ccaD(X)= 0.855886, D(G(X))= 0.384589\n",
            "Epoch: 15, d_loss= 0.804497, g_loss= 24.989340, ccaD(X)= 0.836014, D(G(X))= 0.310081\n",
            "Epoch: 15, d_loss= 0.665297, g_loss= 23.744074, ccaD(X)= 0.653978, D(G(X))= 0.129233\n",
            "Epoch: 15, d_loss= 0.823768, g_loss= 25.014763, ccaD(X)= 0.771646, D(G(X))= 0.326705\n",
            "Epoch: 15, d_loss= 0.821556, g_loss= 25.607691, ccaD(X)= 0.821362, D(G(X))= 0.340737\n",
            "Epoch: 16, d_loss= 1.261809, g_loss= 23.060743, ccaD(X)= 0.361486, D(G(X))= 0.092108\n",
            "Epoch: 16, d_loss= 0.790239, g_loss= 24.017069, ccaD(X)= 0.818148, D(G(X))= 0.311999\n",
            "Epoch: 16, d_loss= 0.538319, g_loss= 25.645699, ccaD(X)= 0.809971, D(G(X))= 0.143312\n",
            "Epoch: 16, d_loss= 0.688674, g_loss= 25.410816, ccaD(X)= 0.804764, D(G(X))= 0.249512\n",
            "Epoch: 16, d_loss= 0.799640, g_loss= 24.054672, ccaD(X)= 0.551246, D(G(X))= 0.109632\n",
            "Epoch: 16, d_loss= 0.718404, g_loss= 24.147474, ccaD(X)= 0.592188, D(G(X))= 0.109620\n",
            "Epoch: 16, d_loss= 0.839657, g_loss= 22.964062, ccaD(X)= 0.512842, D(G(X))= 0.102737\n",
            "Epoch: 16, d_loss= 0.989416, g_loss= 23.444246, ccaD(X)= 0.431011, D(G(X))= 0.089934\n",
            "Epoch: 16, d_loss= 0.881057, g_loss= 26.289673, ccaD(X)= 0.855617, D(G(X))= 0.378472\n",
            "Epoch: 16, d_loss= 0.708280, g_loss= 24.156464, ccaD(X)= 0.596828, D(G(X))= 0.101931\n",
            "Epoch: 16, d_loss= 0.911407, g_loss= 24.153095, ccaD(X)= 0.795619, D(G(X))= 0.364373\n",
            "Epoch: 16, d_loss= 0.658291, g_loss= 24.391781, ccaD(X)= 0.704896, D(G(X))= 0.167553\n",
            "Epoch: 16, d_loss= 0.866180, g_loss= 24.314692, ccaD(X)= 0.532661, D(G(X))= 0.155410\n",
            "Epoch: 16, d_loss= 0.847327, g_loss= 23.484207, ccaD(X)= 0.491390, D(G(X))= 0.068666\n",
            "Epoch: 16, d_loss= 0.882486, g_loss= 22.951094, ccaD(X)= 0.534634, D(G(X))= 0.162365\n",
            "Epoch: 16, d_loss= 0.693677, g_loss= 24.254707, ccaD(X)= 0.713288, D(G(X))= 0.195700\n",
            "Epoch: 16, d_loss= 0.785795, g_loss= 24.667522, ccaD(X)= 0.783962, D(G(X))= 0.301200\n",
            "Epoch: 16, d_loss= 0.852588, g_loss= 25.246082, ccaD(X)= 0.829666, D(G(X))= 0.361730\n",
            "Epoch: 16, d_loss= 0.980434, g_loss= 23.245829, ccaD(X)= 0.410597, D(G(X))= 0.041452\n",
            "Epoch: 16, d_loss= 0.755020, g_loss= 25.625462, ccaD(X)= 0.897852, D(G(X))= 0.312479\n",
            "Epoch: 16, d_loss= 0.793651, g_loss= 24.929956, ccaD(X)= 0.825681, D(G(X))= 0.326093\n",
            "Epoch: 16, d_loss= 0.725005, g_loss= 24.229847, ccaD(X)= 0.825831, D(G(X))= 0.282512\n",
            "Epoch: 16, d_loss= 0.739148, g_loss= 26.269686, ccaD(X)= 0.872868, D(G(X))= 0.303427\n",
            "Epoch: 17, d_loss= 0.984106, g_loss= 22.854074, ccaD(X)= 0.415212, D(G(X))= 0.073000\n",
            "Epoch: 17, d_loss= 0.770066, g_loss= 24.387714, ccaD(X)= 0.696358, D(G(X))= 0.230669\n",
            "Epoch: 17, d_loss= 0.820660, g_loss= 26.630188, ccaD(X)= 0.889582, D(G(X))= 0.358097\n",
            "Epoch: 17, d_loss= 1.258579, g_loss= 22.825424, ccaD(X)= 0.315911, D(G(X))= 0.032380\n",
            "Epoch: 17, d_loss= 0.670735, g_loss= 25.073107, ccaD(X)= 0.757425, D(G(X))= 0.208950\n",
            "Epoch: 17, d_loss= 0.660068, g_loss= 25.171619, ccaD(X)= 0.841887, D(G(X))= 0.247543\n",
            "Epoch: 17, d_loss= 0.674793, g_loss= 23.553900, ccaD(X)= 0.651689, D(G(X))= 0.136915\n",
            "Epoch: 17, d_loss= 0.730968, g_loss= 24.374491, ccaD(X)= 0.817119, D(G(X))= 0.284911\n",
            "Epoch: 17, d_loss= 0.666571, g_loss= 25.911572, ccaD(X)= 0.863245, D(G(X))= 0.254919\n",
            "Epoch: 17, d_loss= 0.733647, g_loss= 24.020414, ccaD(X)= 0.704964, D(G(X))= 0.219285\n",
            "Epoch: 17, d_loss= 1.575464, g_loss= 22.522060, ccaD(X)= 0.229678, D(G(X))= 0.025995\n",
            "Epoch: 17, d_loss= 0.632822, g_loss= 24.955477, ccaD(X)= 0.825000, D(G(X))= 0.209579\n",
            "Epoch: 17, d_loss= 0.696858, g_loss= 23.837833, ccaD(X)= 0.636309, D(G(X))= 0.142320\n",
            "Epoch: 17, d_loss= 0.657625, g_loss= 26.323690, ccaD(X)= 0.878023, D(G(X))= 0.252240\n",
            "Epoch: 17, d_loss= 0.778565, g_loss= 24.394947, ccaD(X)= 0.790832, D(G(X))= 0.299533\n",
            "Epoch: 17, d_loss= 0.584421, g_loss= 25.246990, ccaD(X)= 0.705584, D(G(X))= 0.107983\n",
            "Epoch: 17, d_loss= 0.590349, g_loss= 24.455034, ccaD(X)= 0.780921, D(G(X))= 0.167930\n",
            "Epoch: 17, d_loss= 0.749883, g_loss= 23.728668, ccaD(X)= 0.584839, D(G(X))= 0.102502\n",
            "Epoch: 17, d_loss= 0.660363, g_loss= 26.094933, ccaD(X)= 0.856805, D(G(X))= 0.245997\n",
            "Epoch: 17, d_loss= 0.740356, g_loss= 25.080576, ccaD(X)= 0.870733, D(G(X))= 0.301933\n",
            "Epoch: 17, d_loss= 0.660397, g_loss= 24.350538, ccaD(X)= 0.628137, D(G(X))= 0.097015\n",
            "Epoch: 17, d_loss= 1.736508, g_loss= 28.019846, ccaD(X)= 0.931192, D(G(X))= 0.703681\n",
            "Epoch: 17, d_loss= 0.630305, g_loss= 24.769482, ccaD(X)= 0.675491, D(G(X))= 0.117505\n",
            "Epoch: 18, d_loss= 0.884495, g_loss= 26.441013, ccaD(X)= 0.934907, D(G(X))= 0.378905\n",
            "Epoch: 18, d_loss= 0.640284, g_loss= 24.620747, ccaD(X)= 0.677233, D(G(X))= 0.122772\n",
            "Epoch: 18, d_loss= 0.659466, g_loss= 24.474377, ccaD(X)= 0.738463, D(G(X))= 0.186064\n",
            "Epoch: 18, d_loss= 0.666815, g_loss= 24.472872, ccaD(X)= 0.681188, D(G(X))= 0.150688\n",
            "Epoch: 18, d_loss= 1.309557, g_loss= 23.255215, ccaD(X)= 0.283107, D(G(X))= 0.027848\n",
            "Epoch: 18, d_loss= 0.911086, g_loss= 23.759867, ccaD(X)= 0.438355, D(G(X))= 0.040194\n",
            "Epoch: 18, d_loss= 0.609425, g_loss= 25.142443, ccaD(X)= 0.715904, D(G(X))= 0.137204\n",
            "Epoch: 18, d_loss= 0.800856, g_loss= 22.995968, ccaD(X)= 0.550524, D(G(X))= 0.116371\n",
            "Epoch: 18, d_loss= 0.621899, g_loss= 25.439720, ccaD(X)= 0.789888, D(G(X))= 0.188399\n",
            "Epoch: 18, d_loss= 0.649934, g_loss= 24.229551, ccaD(X)= 0.611449, D(G(X))= 0.067708\n",
            "Epoch: 18, d_loss= 0.834966, g_loss= 23.380045, ccaD(X)= 0.482863, D(G(X))= 0.050567\n",
            "Epoch: 18, d_loss= 0.636776, g_loss= 24.682056, ccaD(X)= 0.850221, D(G(X))= 0.229728\n",
            "Epoch: 18, d_loss= 0.650890, g_loss= 23.981026, ccaD(X)= 0.708302, D(G(X))= 0.166627\n",
            "Epoch: 18, d_loss= 0.734726, g_loss= 24.275536, ccaD(X)= 0.836267, D(G(X))= 0.286378\n",
            "Epoch: 18, d_loss= 0.634120, g_loss= 24.852501, ccaD(X)= 0.716551, D(G(X))= 0.153470\n",
            "Epoch: 18, d_loss= 2.361185, g_loss= 22.634624, ccaD(X)= 0.106731, D(G(X))= 0.013201\n",
            "Epoch: 18, d_loss= 0.664282, g_loss= 24.086487, ccaD(X)= 0.768611, D(G(X))= 0.209989\n",
            "Epoch: 18, d_loss= 0.652569, g_loss= 24.417217, ccaD(X)= 0.744820, D(G(X))= 0.191926\n",
            "Epoch: 18, d_loss= 0.713212, g_loss= 24.948656, ccaD(X)= 0.819005, D(G(X))= 0.266205\n",
            "Epoch: 18, d_loss= 0.721896, g_loss= 25.760048, ccaD(X)= 0.871963, D(G(X))= 0.281526\n",
            "Epoch: 18, d_loss= 0.752742, g_loss= 23.918055, ccaD(X)= 0.536209, D(G(X))= 0.069768\n",
            "Epoch: 18, d_loss= 0.761976, g_loss= 23.343908, ccaD(X)= 0.554861, D(G(X))= 0.099822\n",
            "Epoch: 18, d_loss= 0.592108, g_loss= 24.381729, ccaD(X)= 0.739212, D(G(X))= 0.137892\n",
            "Epoch: 19, d_loss= 0.565811, g_loss= 24.838400, ccaD(X)= 0.683319, D(G(X))= 0.071692\n",
            "Epoch: 19, d_loss= 0.682149, g_loss= 25.974602, ccaD(X)= 0.881313, D(G(X))= 0.266729\n",
            "Epoch: 19, d_loss= 0.903848, g_loss= 23.413326, ccaD(X)= 0.603909, D(G(X))= 0.241023\n",
            "Epoch: 19, d_loss= 0.648997, g_loss= 24.185114, ccaD(X)= 0.731609, D(G(X))= 0.174944\n",
            "Epoch: 19, d_loss= 0.658065, g_loss= 23.893932, ccaD(X)= 0.697142, D(G(X))= 0.152657\n",
            "Epoch: 19, d_loss= 0.657283, g_loss= 24.227095, ccaD(X)= 0.631472, D(G(X))= 0.085258\n",
            "Epoch: 19, d_loss= 0.537924, g_loss= 25.234396, ccaD(X)= 0.818379, D(G(X))= 0.142813\n",
            "Epoch: 19, d_loss= 0.559679, g_loss= 25.380819, ccaD(X)= 0.727359, D(G(X))= 0.104445\n",
            "Epoch: 19, d_loss= 0.594414, g_loss= 24.155602, ccaD(X)= 0.726595, D(G(X))= 0.127495\n",
            "Epoch: 19, d_loss= 0.595811, g_loss= 25.110855, ccaD(X)= 0.769087, D(G(X))= 0.166640\n",
            "Epoch: 19, d_loss= 0.616056, g_loss= 24.931171, ccaD(X)= 0.794723, D(G(X))= 0.197498\n",
            "Epoch: 19, d_loss= 2.129380, g_loss= 22.877607, ccaD(X)= 0.124100, D(G(X))= 0.014528\n",
            "Epoch: 19, d_loss= 0.623182, g_loss= 25.007454, ccaD(X)= 0.642481, D(G(X))= 0.060167\n",
            "Epoch: 19, d_loss= 0.701626, g_loss= 25.243732, ccaD(X)= 0.905080, D(G(X))= 0.280806\n",
            "Epoch: 19, d_loss= 0.588124, g_loss= 25.432909, ccaD(X)= 0.703491, D(G(X))= 0.108439\n",
            "Epoch: 19, d_loss= 0.685661, g_loss= 24.547403, ccaD(X)= 0.771628, D(G(X))= 0.218900\n",
            "Epoch: 19, d_loss= 0.745987, g_loss= 24.619576, ccaD(X)= 0.556754, D(G(X))= 0.066917\n",
            "Epoch: 19, d_loss= 0.672977, g_loss= 25.093483, ccaD(X)= 0.816278, D(G(X))= 0.241187\n",
            "Epoch: 19, d_loss= 0.667535, g_loss= 24.658453, ccaD(X)= 0.634609, D(G(X))= 0.102904\n",
            "Epoch: 19, d_loss= 0.630623, g_loss= 25.120474, ccaD(X)= 0.655866, D(G(X))= 0.104282\n",
            "Epoch: 19, d_loss= 0.642409, g_loss= 23.702965, ccaD(X)= 0.688245, D(G(X))= 0.139968\n",
            "Epoch: 19, d_loss= 0.915783, g_loss= 23.114122, ccaD(X)= 0.447654, D(G(X))= 0.066603\n",
            "Epoch: 19, d_loss= 0.601915, g_loss= 25.254396, ccaD(X)= 0.756713, D(G(X))= 0.148168\n",
            "*** Calling test() ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-fbe72eee22d5>:181: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  right_images_v = Variable(right_images.float(), volatile=True)\n",
            "<ipython-input-17-fbe72eee22d5>:182: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  right_embed_v = Variable(right_embed.float(), volatile=True)\n",
            "<ipython-input-17-fbe72eee22d5>:186: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  noise = Variable(torch.randn(right_images_v.size(0), self.noise_dim), volatile=True)\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.90659726..0.8904159].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fake_source shape:  torch.Size([128, 3, 64, 64])\n",
            "text description:  this flower has multiple layers of white petals with spiky yellow stamen at the center.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-fbe72eee22d5>:181: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  right_images_v = Variable(right_images.float(), volatile=True)\n",
            "<ipython-input-17-fbe72eee22d5>:182: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  right_embed_v = Variable(right_embed.float(), volatile=True)\n",
            "<ipython-input-17-fbe72eee22d5>:186: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  noise = Variable(torch.randn(right_images_v.size(0), self.noise_dim), volatile=True)\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9375179..0.91146946].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fake_source shape:  torch.Size([128, 3, 64, 64])\n",
            "text description:  this flower has multiple layers of white petals with spiky yellow stamen at the center.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "combined text:  0 original sentence:  this flower has multiple layers of white petals with spiky yellow stamen at the center.\n",
            " nn_sentences:  ['this is a large, white flower with large petals and yellow stamen.\\n', 'this flower is purple in color, and has petals that are very skinny.\\n']\n",
            "\n",
            "combined text:  1 original sentence:  this flower is white in color, with petals that are wavy along the edges.\n",
            " nn_sentences:  ['this flower is purple in color, with petals that are bell shaped.\\n', 'this flower has four large pink petals which are slightly heart shaped and have leaflike veins.\\n']\n",
            "\n",
            "combined text:  2 original sentence:  this flower has wide trumpet shaped blossoms in pale pink and white hues.\n",
            " nn_sentences:  ['this flower has petals that are pink and has patch of yellow and dark lines\\n', 'this flower is purple in color, with petals that are small stars.\\n']\n",
            "\n",
            "combined text:  3 original sentence:  this flower has a petal that has brown spots and has green anther filaments\n",
            " nn_sentences:  ['this white flower has a white stigma with an green pedicel and lime sepal.\\n', 'this flower has a large number of very thin petals that are purple in color and arranged upright.\\n']\n",
            "\n",
            "combined text:  4 original sentence:  this flower has small and rounded orange petals with darker orange veins.\n",
            " nn_sentences:  ['this flower has very thick and very pointed petals in a bright yellow hue.\\n', 'this flower is pink and orange in color, with petals that are curled and wavy.\\n']\n",
            "\n",
            "combined text:  5 original sentence:  this flower has petals that are purple with white shading\n",
            " nn_sentences:  ['this particular flower has petals that are long and white with a yellow center\\n', 'this flower has a large number of very thin petals that are purple in color and arranged upright.\\n']\n",
            "\n",
            "combined text:  6 original sentence:  this flower has multiple layers of pale purple petals with a yellow center and purple stamen.\n",
            " nn_sentences:  ['a flower with no visible petals and purple pistils in the center.\\n', 'this flower has a row of pink pointed petals and many pistils and stamens.\\n']\n",
            "\n",
            "combined text:  7 original sentence:  this flower has petals that are pink and very thin\n",
            " nn_sentences:  ['this orange and dark purple spotted flower has curled petals and pale orange and burgundy stamen.\\n', 'the visible petals on the plant are thin and purple while the stem has spikes on it\\n']\n",
            "\n",
            "combined text:  8 original sentence:  this flower has large white petals and long yellow stamens.\n",
            " nn_sentences:  ['the petals of this flower are green with a long stigma\\n', 'this flower has very many small and spiky green sepals underneath a disk-shaped collection of thin purple petals.\\n']\n",
            "\n",
            "combined text:  9 original sentence:  the petals of this flower are red with a long stigma\n",
            " nn_sentences:  ['this flower is purple and green in color, with petals that are pointed.\\n', 'a flower with no visible petals and purple pistils in the center.\\n']\n",
            "\n",
            "combined text:  10 original sentence:  this flower has petals that are red and has a yellow center\n",
            " nn_sentences:  ['this flower has petals that are yellow and are very thin\\n', 'this flower has white petals as well as a purple pistil.\\n']\n",
            "\n",
            "combined text:  11 original sentence:  this flower has bright pink pointed petal and green pedicel and receptacle.\n",
            " nn_sentences:  ['a multi-tiered yellow flower with large upward pointing curling petals and yellow anther.\\n', 'this flower has petals that are white and has a yellow center\\n']\n",
            "\n",
            "combined text:  12 original sentence:  this flower is pink in color, and has petals that are horn shaped.\n",
            " nn_sentences:  ['this flower has a large number of very thin petals that are purple in color and arranged upright.\\n', 'this flower has pink and yellow petals with brown specks on them.\\n']\n",
            "\n",
            "combined text:  13 original sentence:  this large flower has a fuchsia center and five large, pale purple petals with veins and ruffled edges.\n",
            " nn_sentences:  ['this flower is yellow and red in color, with wavy and wrinkled petals.\\n', 'this flower has a green sepal and round blub-like purple petals.\\n']\n",
            "\n",
            "combined text:  14 original sentence:  white flower has six petals and yellow anthers.\n",
            " nn_sentences:  ['this flower has long and pointy red petals and a group of white stamen in the cener\\n', 'this flower has four off white and purpley-pink petals, one sticking out of each side, and then a flat yellow white and pink pistil.\\n']\n",
            "\n",
            "combined text:  15 original sentence:  this flower has pale pink sepals and long flowers with small white petals.\n",
            " nn_sentences:  ['this flower has petals that are green and has purple stamen\\n', 'this flower has pink petals as well as a yellow stamen.\\n']\n",
            "\n",
            "combined text:  16 original sentence:  this flower has long and thin yellow petals and anthers in the middle\n",
            " nn_sentences:  ['the round purple flower has many pointed petals that look like like a lighter shade of purple.\\n', 'this flower has pointy green petals as its main feature\\n']\n",
            "\n",
            "combined text:  17 original sentence:  this flower is white and yellow in color, with petals that are ruffled at the edges.\n",
            " nn_sentences:  ['this flower has long purple ribbed petals in a ring configuration.\\n', 'this flower has purple pestals on top and a geen pricky on bottom like a catus\\n']\n",
            "\n",
            "combined text:  18 original sentence:  this flower has purple petals as well as a green sepal.\n",
            " nn_sentences:  ['the petals of the flower are curved upwards, and has a pointed edge.\\n', 'this flower has a large number of very thin petals that are purple in color and arranged upright.\\n']\n",
            "\n",
            "combined text:  19 original sentence:  a large pink flower and a golden and white center.\n",
            " nn_sentences:  ['this flower has a large number of very thin petals that are purple in color and arranged upright.\\n', 'this flower has petals that are green and has purple stamen\\n']\n",
            "\n",
            "combined text:  20 original sentence:  this flower has deep purple petals and one petal that has a yellow spot and brown lines.\n",
            " nn_sentences:  ['this is a large, orange flower with blue spots on the petals and orange stamen.\\n', 'the petals of the flower is purple with a green sepal that it is sitting on.\\n']\n",
            "\n",
            "combined text:  21 original sentence:  bright orange pointed overlapping petals with a dark yellow pistil.\n",
            " nn_sentences:  ['the flower has petals that are green and spiky with pink filaments.\\n', 'this flower is white and purple in color, with petals that are spotted.\\n']\n",
            "\n",
            "combined text:  22 original sentence:  white and light pink pistil surrounded by large, smooth pink petals.\n",
            " nn_sentences:  ['this flower is round and has pointed purple petals with green pointed sepals.\\n', 'a flower with a lot of pistils that are purple and pointed.\\n']\n",
            "\n",
            "combined text:  23 original sentence:  the flower has white petals that are wavy at the edges.\n",
            " nn_sentences:  ['this flower has multiple pink with dark pink markings petals and pink with dark pink and yellow and white petals.\\n', 'this flower is purple and green in color, with petals that are pointed.\\n']\n",
            "\n",
            "combined text:  24 original sentence:  this flower has small red blossoms whose petals are smooth and very round.\n",
            " nn_sentences:  ['this flower has bright and very spiky, pointed petals in vivid yellow hues.\\n', 'the flower has bell shaped purple petals with white stamen\\n']\n",
            "\n",
            "combined text:  25 original sentence:  the flower shown has purple petals with spots of dark brown on them\n",
            " nn_sentences:  ['a beautiful strong green stem stands tall and attached to a bright yellow layered petals sitting above the sepal.\\n', 'this flower has green petals as well as a green pedicel.\\n']\n",
            "\n",
            "combined text:  26 original sentence:  the petals of the flower are pink in color and have filaments that are pink in color.\n",
            " nn_sentences:  ['this flower is blue in color, and has petals that are very small.\\n', 'this flower is green and purple in color, with petals that are pointed.\\n']\n",
            "\n",
            "combined text:  27 original sentence:  these bell shaped light purple flowers have black spots inside them.\n",
            " nn_sentences:  ['a flower with long and wide petals that are dark pink with brown stripes.\\n', 'this flower is yellow in color, with petals that are skinny and long.\\n']\n",
            "\n",
            "combined text:  28 original sentence:  this flower has red wrinkled petals and red stringy stamen.\n",
            " nn_sentences:  ['the tropical flower has orange blooms that grow horizontally from the branch of the flower.\\n', 'this flower has white petals as well as a white pistil.\\n']\n",
            "\n",
            "combined text:  29 original sentence:  this flower has with petals blooming on a green bus of leaves.\n",
            " nn_sentences:  ['this flower has a row of pink pointed petals and many pistils and stamens.\\n', 'a flower with no visible petals and purple pistils in the center.\\n']\n",
            "\n",
            "combined text:  30 original sentence:  this flower is pink and yellow in color, with petals that are darker at the tips.\n",
            " nn_sentences:  ['flowers has spiky needle shapd green petals,with perple stamen\\n', 'this flower has petals that are green and has purple stamen\\n']\n",
            "\n",
            "combined text:  31 original sentence:  this flower has petals that are red with yellow stamen\n",
            " nn_sentences:  ['this unique flower features draping stamen and polka-dotted petals.\\n', 'this flower has petals that are white and has a yellow center\\n']\n",
            "\n",
            "combined text:  32 original sentence:  a flower with white petals, yellow stamen, and a green pedicel.\n",
            " nn_sentences:  ['this flower has petals that are green and has purple stamen\\n', 'this flower has many spiky green leaves along with a bunch of very thin purple petals.\\n']\n",
            "\n",
            "combined text:  33 original sentence:  this flower has a large light blue petal that gets white in the center and gives the flower a trumpet like appearance.\n",
            " nn_sentences:  ['this flower consists of a large, white petal that wraps completely around the stem.\\n', 'the petals on this flower are purple with small white stamen.\\n']\n",
            "\n",
            "combined text:  34 original sentence:  this flower is purple in color, and has petals that are yellow near the center.\n",
            " nn_sentences:  ['this flower has a few amount of petals that are a very dark shade of purple.\\n', 'this flower has petals that are purple with green patches\\n']\n",
            "\n",
            "combined text:  35 original sentence:  the petals on this flower are white with green veins.\n",
            " nn_sentences:  ['this flower has petals that are purple and very flat\\n', 'this flower has petals that are red with black dots\\n']\n",
            "\n",
            "combined text:  36 original sentence:  this flower is a lavender cupped petal with the center being white and purple spots.\n",
            " nn_sentences:  ['each part of the flower, including the petals, pistil, stamen, sepals are all varying shades of dark to light green.\\n', 'the flower has a few thick and sharp orange colored petals.\\n']\n",
            "\n",
            "combined text:  37 original sentence:  this flower is orange and yellow in color, with petals that are pointed on the ends.\n",
            " nn_sentences:  ['a flower with long and wide petals that curl inwards.\\n', 'this is a yellow flower with a long green pedicel.\\n']\n",
            "\n",
            "combined text:  38 original sentence:  this flower has petals that are yellow and has red stripes\n",
            " nn_sentences:  ['this flower is yellow in color, and has petals that are curved inward.\\n', 'this flower has petals that are pink and has green stamen\\n']\n",
            "\n",
            "combined text:  39 original sentence:  this flower look like a wheat blossom with yellow and maroon tiny petals.\n",
            " nn_sentences:  ['this flower is yellow in color, and has petals that are very skinny and long.\\n', 'this flower has green petals as well as a green pedicel.\\n']\n",
            "\n",
            "combined text:  40 original sentence:  this flower is pink in color, and has petals wavy and thin.\n",
            " nn_sentences:  ['this flower is purple in color, and has petals that are very skinny.\\n', 'flowers has spiky needle shapd green petals,with perple stamen\\n']\n",
            "\n",
            "combined text:  41 original sentence:  has many stamens that are white in color with yellow anthers\n",
            " nn_sentences:  ['this flower has long pink petals with brown streaks on the middle, and black anthers coming out\\n', 'this flower consists of a large, white petal that wraps completely around the stem.\\n']\n",
            "\n",
            "combined text:  42 original sentence:  this flower has bright red petals that are all standing on end.\n",
            " nn_sentences:  ['this is a yellow flower with a long green pedicel.\\n', 'flowers has spiky needle shapd green petals,with perple stamen\\n']\n",
            "\n",
            "combined text:  43 original sentence:  this flower has petals that are white with purple stamen\n",
            " nn_sentences:  ['the flower has a few thick and sharp orange colored petals.\\n', 'this flower has white petals as well as a white pistil.\\n']\n",
            "\n",
            "combined text:  44 original sentence:  this flower has many trumpet shaped pink blossoms with spots on the insides of each petal.\n",
            " nn_sentences:  ['this flower has orange petals that curly outward and are spotted with purple, with long purple stamen.\\n', 'a beautiful strong green stem stands tall and attached to a bright yellow layered petals sitting above the sepal.\\n']\n",
            "\n",
            "combined text:  45 original sentence:  this is a bright yellow flower with round petals and long yellow anthers.\n",
            " nn_sentences:  ['a flower with a lot of pistils that are purple and pointed.\\n', 'the petals of this flower are green with a long stigma\\n']\n",
            "\n",
            "combined text:  46 original sentence:  this flower has petals that are pink and has purple lines\n",
            " nn_sentences:  ['this flower has petals in the shape of a ball and are very thorny\\n', 'the flower has bell shaped purple petals with white stamen\\n']\n",
            "\n",
            "combined text:  47 original sentence:  the petals are white, rounded, and layered on top of one another, and the stamen are many and yellow.\n",
            " nn_sentences:  ['this flower has four off white and purpley-pink petals, one sticking out of each side, and then a flat yellow white and pink pistil.\\n', 'this flower has four off white and purpley-pink petals, one sticking out of each side, and then a flat yellow white and pink pistil.\\n']\n",
            "\n",
            "combined text:  48 original sentence:  this flower has a wide yellow center surrounded by long slightly tapered yellow and red petals.\n",
            " nn_sentences:  ['the flower has many small pointy yellow petals with yellow anther in the center\\n', 'the flower has many small pointy yellow petals with yellow anther in the center\\n']\n",
            "\n",
            "combined text:  49 original sentence:  the petals on this flower are pink with yellow stamen.\n",
            " nn_sentences:  ['this flower has a row of pink pointed petals and many pistils and stamens.\\n', 'this flower has petals that are yellow and very stringy\\n']\n",
            "\n",
            "combined text:  50 original sentence:  this flower has petals that are red and are ruffled at the edges\n",
            " nn_sentences:  ['this flower is purple in color, and has petals that are very skinny.\\n', 'this is a yellowish orange flower with many thin petals and a green, almost furry stem.\\n']\n",
            "\n",
            "combined text:  51 original sentence:  the flower is yellow with petals that are soft, smooth, thin and separately arranged around stamens\n",
            " nn_sentences:  ['this flower has pink petals as well as a yellow stamen.\\n', 'this flower has very many small and spiky green sepals underneath a disk-shaped collection of thin purple petals.\\n']\n",
            "\n",
            "combined text:  52 original sentence:  a bright pink petaled flower with a pink stamen.\n",
            " nn_sentences:  ['this flower is purple in color, and has petals that are very skinny.\\n', 'this flower has a large number of very thin petals that are purple in color and arranged upright.\\n']\n",
            "\n",
            "combined text:  53 original sentence:  the flower has pinkish purple color on its petals with its filaments' tips in yellow color.\n",
            " nn_sentences:  ['this flower is purple in color, and has petals that are very skinny.\\n', 'a flower with no visible petals and purple pistils in the center.\\n']\n",
            "\n",
            "combined text:  54 original sentence:  this red flower has overlapping petals and a few yellow stamens.\n",
            " nn_sentences:  ['this flower has four off white and purpley-pink petals, one sticking out of each side, and then a flat yellow white and pink pistil.\\n', 'this flower has petals that are yellow and very stringy\\n']\n",
            "\n",
            "combined text:  55 original sentence:  this flower has petals that hae jagged edges and are pink and white with long stamen of yellow and orangish red.\n",
            " nn_sentences:  ['this flower has pink petals and has red and green stamen\\n', 'this flower has spiky red petals,coming from a base that has several spiky stigma coming from it.\\n']\n",
            "\n",
            "combined text:  56 original sentence:  this flower has a trumpet appearance with white petals and short, pale yellow stamens.\n",
            " nn_sentences:  ['this flower is purple in color, and has petals that are very skinny.\\n', 'this flower has a yellow center and long, thin rounded yellow petals.\\n']\n",
            "\n",
            "combined text:  57 original sentence:  this is a strange flower with white petals and yellow stamen.\n",
            " nn_sentences:  ['this flower has orange petals with stamen of the same color, and dark red marks on a few of the petals.\\n', 'this flower has long purple stamen and no visible anthers on the stamen\\n']\n",
            "\n",
            "combined text:  58 original sentence:  many smooth spoon shaped white petals create an open ball shaped flower with clusters of yellow stamen in the center.\n",
            " nn_sentences:  ['this flower has petals that are punk with purple lines\\n', 'the petals are yellow with anthers appearing as white.\\n']\n",
            "\n",
            "combined text:  59 original sentence:  a flower with long and large white petals with pink overtunes\n",
            " nn_sentences:  ['this flower is green and purple in color, with petals that are pointed.\\n', 'this flower has petals that are white and has a yellow center\\n']\n",
            "\n",
            "combined text:  60 original sentence:  this flower has a pink ovary with a generally cupped arrangement of smooth oblong white petals.\n",
            " nn_sentences:  ['this flower has a row of pink pointed petals and many pistils and stamens.\\n', 'there are two flies enjoying the sweet nectar of this lavender wild flower.\\n']\n",
            "\n",
            "combined text:  61 original sentence:  this flower has petals that are yellow and very stringy with yellow steman\n",
            " nn_sentences:  ['this flower has purple pestals on top and a geen pricky on bottom like a catus\\n', 'this flower has long, tapered lavender petals around prominent brown stamen.\\n']\n",
            "\n",
            "combined text:  62 original sentence:  this flower has five large orange petals with slightly rounded edges.\n",
            " nn_sentences:  ['this flower has pink petals as well as a yellow stamen.\\n', 'this flower has pink petals as well as a yellow stamen.\\n']\n",
            "\n",
            "combined text:  63 original sentence:  this flower has petals that are purple and has a yellow center\n",
            " nn_sentences:  ['this flower consists of a large, white petal that wraps completely around the stem.\\n', 'this is a large, orange flower with blue spots on the petals and orange stamen.\\n']\n",
            "\n",
            "combined text:  64 original sentence:  this flower buds into five white petals with a yellow center and has a red stem.\n",
            " nn_sentences:  ['this flower is green and pink in color, with petals that are spikey.\\n', 'the flower has many small pointy yellow petals with yellow anther in the center\\n']\n",
            "\n",
            "combined text:  65 original sentence:  the flower has light and dark pink petals, pink pistil, and stamen.\n",
            " nn_sentences:  ['this flower is yellow in color, and has petals that are drooping.\\n', 'this flower has petals that are white with a long style\\n']\n",
            "\n",
            "combined text:  66 original sentence:  this flower is purple in color, and has petals that are ruffled and thin.\n",
            " nn_sentences:  ['the visible petals on the plant are thin and purple while the stem has spikes on it\\n', 'this flower is pink and white in color, with petals that are pointed at the ends.\\n']\n",
            "\n",
            "combined text:  67 original sentence:  round flower in shades of pink petals\n",
            " nn_sentences:  ['this flower has petals that are green and has purple stamen\\n', 'the flower has many small pointy yellow petals with yellow anther in the center\\n']\n",
            "\n",
            "combined text:  68 original sentence:  a large flower with a dark brown and a bright yellow pedal coloration.\n",
            " nn_sentences:  ['this flower has yellow center and a fringe of very thin yellow hair-like petals.\\n', 'this flower is pink and white in color, with petals that are pointed upward.\\n']\n",
            "\n",
            "combined text:  69 original sentence:  flower with many long white petals and yellow stamen\n",
            " nn_sentences:  ['this orange and dark purple spotted flower has curled petals and pale orange and burgundy stamen.\\n', 'this flower has white petals as well as a white pistil.\\n']\n",
            "\n",
            "combined text:  70 original sentence:  this flower has white petals and darker pink/red stamen.\n",
            " nn_sentences:  ['this flower has petals that are green and has purple stamen\\n', 'this flower has a dense amount of small spiky purple petals.\\n']\n",
            "\n",
            "combined text:  71 original sentence:  the petals on this flower are yellow with a white fringe.\n",
            " nn_sentences:  ['this flower has wide and very smooth white petals and a yellow mouth.\\n', 'this flower has purple pestals on top and a geen pricky on bottom like a catus\\n']\n",
            "\n",
            "combined text:  72 original sentence:  this flower has numerous overlapping layers of slightly tapered peach and yellow petals.\n",
            " nn_sentences:  ['the petals on this flower are mostly dark purple with a strong green stem.\\n', 'the flower has many small pointy yellow petals with yellow anther in the center\\n']\n",
            "\n",
            "combined text:  73 original sentence:  this flower has a yellow center and wide, tapered orange petals.\n",
            " nn_sentences:  ['a flower with a lot of pistils that are purple and pointed.\\n', 'this flower has varied petals, three of which are rounded with pink stripes on a yellow background and one larger rounded petal that is solid light pink.\\n']\n",
            "\n",
            "combined text:  74 original sentence:  this flower is orange and yellow in color, with petals that are oval shaped.\n",
            " nn_sentences:  ['the flower shown has orange petals with a red and green pedicel\\n', 'this flower consists of a large, white petal that wraps completely around the stem.\\n']\n",
            "\n",
            "combined text:  75 original sentence:  this white flower with green leaves has four lines of purple along its axis.\n",
            " nn_sentences:  ['this flower has a large number of very thin petals that are purple in color and arranged upright.\\n', 'a flower with long and wide petals that curl inwards.\\n']\n",
            "\n",
            "combined text:  76 original sentence:  this flower has petals that open out flat and are orange veined with dark orange and yellow at edges.\n",
            " nn_sentences:  ['this flower has a ball of textured purple buds producing tiny flowers.\\n', 'this flower has a long yellow stamen and a white petal with pointed tip.\\n']\n",
            "\n",
            "combined text:  77 original sentence:  this flower has a red petal that has a big white style\n",
            " nn_sentences:  ['this flower has four off white and purpley-pink petals, one sticking out of each side, and then a flat yellow white and pink pistil.\\n', 'this flower has white petals as well as a white pistil.\\n']\n",
            "\n",
            "combined text:  78 original sentence:  this flower has white veined leaves surrounding a tiny white blossom.\n",
            " nn_sentences:  ['this flower consists of a large, white petal that wraps completely around the stem.\\n', 'this flower has a dome-like configuration of very thin and spiky purple petals.\\n']\n",
            "\n",
            "combined text:  79 original sentence:  this flower is white and red in color, with petals that are oval shaped.\n",
            " nn_sentences:  ['this flower has pink petals as well as a yellow stamen.\\n', 'this flower has white petals as well as a white pistil.\\n']\n",
            "\n",
            "combined text:  80 original sentence:  a flower with large purple leaf like petals surround a prickly white ovule.\n",
            " nn_sentences:  ['this flower has not yet bloomed; there are several rows of petals on the bud.\\n', 'this flower has large, shiny, very pointed yellow petals which angle sharply upwards.\\n']\n",
            "\n",
            "combined text:  81 original sentence:  this flower is white and yellow in color, with petals that are oval shaped.\n",
            " nn_sentences:  ['this flower has drooping pink petals and orange pistil as its main features\\n', 'the flower has many small pointy yellow petals with yellow anther in the center\\n']\n",
            "\n",
            "combined text:  82 original sentence:  this flower is white and purple in color, with petals that are multi shaped.\n",
            " nn_sentences:  ['this flower has a large number of very thin petals that are purple in color and arranged upright.\\n', 'this flower has purple pestals on top and a geen pricky on bottom like a catus\\n']\n",
            "\n",
            "combined text:  83 original sentence:  this yellow and red flower has pointed petals and a green pedicel.\n",
            " nn_sentences:  ['this flower has four off white and purpley-pink petals, one sticking out of each side, and then a flat yellow white and pink pistil.\\n', 'most of the flower is green and pointy, with many purple petals at the very top.\\n']\n",
            "\n",
            "combined text:  84 original sentence:  this flower has petals that are pink and has yellow stamen\n",
            " nn_sentences:  ['this flower has a large number of very thin petals that are purple in color and arranged upright.\\n', 'this flower consists of a large, white petal that wraps completely around the stem.\\n']\n",
            "\n",
            "combined text:  85 original sentence:  this flower has pink petals as well as a green pistil.\n",
            " nn_sentences:  ['the petals are yellow with anthers appearing as white.\\n', 'this flower has white petals as well as a white pistil.\\n']\n",
            "\n",
            "combined text:  86 original sentence:  leaves are green,petals re pink in color with larger anthers\n",
            " nn_sentences:  ['the flower has pointed petals and purple stamen and pistil.\\n', 'this flower is purple in color, and has petals that are very skinny.\\n']\n",
            "\n",
            "combined text:  87 original sentence:  this flower has pink petals that has white and yellow in the center\n",
            " nn_sentences:  ['this flower has petals that are purple with green patches\\n', 'the flower has orange petals with small dark spots, long white filament with oblong black anther.\\n']\n",
            "\n",
            "combined text:  88 original sentence:  this flower has several rows of white petals and a prominent yellow pistil and stamen.\n",
            " nn_sentences:  ['a flower with bright orange petals accompanied by orange anther filaments.\\n', 'this flower has four large pink petals which are slightly heart shaped and have leaflike veins.\\n']\n",
            "\n",
            "combined text:  89 original sentence:  this flower has a thick white stamen and shiny red petal with pointed tip.\n",
            " nn_sentences:  ['this flower has white petals as well as a white pistil.\\n', 'this flower has white petals as well as a white pistil.\\n']\n",
            "\n",
            "combined text:  90 original sentence:  wavy, dry purple petals surround a green to yellow stigma and pistil in the center.\n",
            " nn_sentences:  ['this flower consists of a large, white petal that wraps completely around the stem.\\n', 'this flower has many spiky green leaves along with a bunch of very thin purple petals.\\n']\n",
            "\n",
            "combined text:  91 original sentence:  this flower has orange pistil and pink petals as its main features\n",
            " nn_sentences:  ['this flower has many spiky green leaves along with a bunch of very thin purple petals.\\n', 'this flower is purple in color, and has petals that are very skinny.\\n']\n",
            "\n",
            "combined text:  92 original sentence:  this flower is white and yellow in color, with petals that are very small.\n",
            " nn_sentences:  ['this flower has green petals as well as a green pedicel.\\n', 'a spiky flower with yellow petals and purple inner petals.\\n']\n",
            "\n",
            "combined text:  93 original sentence:  this flower has petals that are white with long stamen\n",
            " nn_sentences:  ['this flower has white petals as well as a white pistil.\\n', 'this flower has a great many very thin yellow petals and a grouping of round yellow pistils at the center.\\n']\n",
            "\n",
            "combined text:  94 original sentence:  flower with long purple petals and long fringed light purple stamen\n",
            " nn_sentences:  ['the leaves are long and dark green, and there are two overlapping rows of pink flower petals with green points surrounding a pistil with green stigma and pink style.\\n', 'a beautiful strong green stem stands tall and attached to a bright yellow layered petals sitting above the sepal.\\n']\n",
            "\n",
            "combined text:  95 original sentence:  this flower has has a bottom layer of white petals and an upper layer of thin, long purple petals and a large green stigma.\n",
            " nn_sentences:  ['a flower with a long and wide petals that is a light purple.\\n', 'this flower has petals that are yellow and are folded together\\n']\n",
            "\n",
            "combined text:  96 original sentence:  this flower is yellow and orange in color, and has petals that are multi colored.\n",
            " nn_sentences:  ['the flower has many small pointy yellow petals with yellow anther in the center\\n', 'this flower has a dense amount of small spiky purple petals.\\n']\n",
            "\n",
            "combined text:  97 original sentence:  this flower has many pink/red petals with pink, red and green sepals.\n",
            " nn_sentences:  ['this flower has a large number of very thin petals that are purple in color and arranged upright.\\n', 'this flower has drooping pink petals and orange pistil as its main features\\n']\n",
            "\n",
            "combined text:  98 original sentence:  the petals on this flower are pink with a yellow center.\n",
            " nn_sentences:  ['this flower is purple in color, and has petals that are very skinny.\\n', 'this flower has a row of pink pointed petals and many pistils and stamens.\\n']\n",
            "\n",
            "combined text:  99 original sentence:  flower with many long spikey petals and stamen\n",
            " nn_sentences:  ['this flower has long purple stamen and no visible anthers on the stamen\\n', 'this flower has purple pestals on top and a geen pricky on bottom like a catus\\n']\n",
            "\n",
            "combined text:  100 original sentence:  this flower has long and thin red petals which protrude upwards.\n",
            " nn_sentences:  ['variegated white and maroon delicate rounded petals with a downward curl.\\n', 'this flower is orange in color, and has petals that are layered.\\n']\n",
            "\n",
            "combined text:  101 original sentence:  there is a dark yellow petal with dark purple stripes on the bottom with two pale yellow petals with dark purple stripes above, and two purple petals on top.\n",
            " nn_sentences:  ['this flower has nice golden yellow petals that have burgundy stripes on them\\n', 'this flower has a white petal encompassing the yellow stamen in the center.\\n']\n",
            "\n",
            "combined text:  102 original sentence:  this flower has petals that are purple with short stamen\n",
            " nn_sentences:  ['this flower is purple in color, and has petals that are very skinny.\\n', 'this flower has red downturned petals and long pink filaments.\\n']\n",
            "\n",
            "combined text:  103 original sentence:  this flower is yellow and white in color, with petals that are rounded on the edges.\n",
            " nn_sentences:  ['this flower has petals that are burgundy with white and yellow in center.\\n', 'the visible petals on the plant are thin and purple while the stem has spikes on it\\n']\n",
            "\n",
            "combined text:  104 original sentence:  this flower is white and yellow in color, and has petals that are oval shaped.\n",
            " nn_sentences:  ['most of the flower is green and pointy, with many purple petals at the very top.\\n', 'this flower has petals that are white and has a yellow center\\n']\n",
            "\n",
            "combined text:  105 original sentence:  this flower has red petals as well as a white stamen.\n",
            " nn_sentences:  ['a flower with no visible petals and purple pistils in the center.\\n', 'this flower has purple pestals on top and a geen pricky on bottom like a catus\\n']\n",
            "\n",
            "combined text:  106 original sentence:  this flower is yellow in color, with petals that are very skinny.\n",
            " nn_sentences:  ['this flower is white and green in color, and has petals that are pointed.\\n', 'this flower has a ball of textured purple buds producing tiny flowers.\\n']\n",
            "\n",
            "combined text:  107 original sentence:  this flower has petals that are yellow and has orange stamen\n",
            " nn_sentences:  ['this particular flower has petals that are long and white with a yellow center\\n', 'this flower has pink petals as well as a yellow stamen.\\n']\n",
            "\n",
            "combined text:  108 original sentence:  a flower with long and wide petals that are blue with yellow centers.\n",
            " nn_sentences:  ['the flower has broader rounder in shape petals that are yellow in color\\n', 'this flower has petals that are purple with green patches\\n']\n",
            "\n",
            "combined text:  109 original sentence:  the flower is made of layers of petals that are yellow in color.\n",
            " nn_sentences:  ['this flower has petals that are purple and very stringy\\n', 'this flower is purple and green in color, with petals that are pointed.\\n']\n",
            "\n",
            "combined text:  110 original sentence:  this flower has white petals as well as a purple stamen.\n",
            " nn_sentences:  ['this flower has pink petals as well as a yellow stamen.\\n', 'this flower has a large number of very thin petals that are purple in color and arranged upright.\\n']\n",
            "\n",
            "combined text:  111 original sentence:  this flower has purple pistil and pink petals as its main features\n",
            " nn_sentences:  ['this flower has pink petals as well as a yellow stamen.\\n', 'this flower is white and pink in color, and has petals that are striped.\\n']\n",
            "\n",
            "combined text:  112 original sentence:  this flower has white fluffy petals with yellow stamen in the center of it.\n",
            " nn_sentences:  ['this flower has purple pestals on top and a geen pricky on bottom like a catus\\n', 'this flower consists of a large, white petal that wraps completely around the stem.\\n']\n",
            "\n",
            "combined text:  113 original sentence:  this purple flower has tall petals and an orange stigma.\n",
            " nn_sentences:  ['this flower has petals that are green and has purple stamen\\n', 'this flower has a large number of very thin petals that are purple in color and arranged upright.\\n']\n",
            "\n",
            "combined text:  114 original sentence:  this flower has wide, rounded purple petals and a thick hairy fringe of purple and white.\n",
            " nn_sentences:  ['this flower has white petals as well as a white pistil.\\n', 'the petals of this flower are green with a long stigma\\n']\n",
            "\n",
            "combined text:  115 original sentence:  the fan shaped petals on this red and white blossom surround several light green stamen, the white stripe running up the center of the petal and the red becoming more vibrant at the tip of the petal.\n",
            " nn_sentences:  ['this flower is green and pink in color, with petals that are pointed.\\n', 'flowers has spiky needle shapd green petals,with perple stamen\\n']\n",
            "\n",
            "combined text:  116 original sentence:  this flower has petals that are yellow and very stringy\n",
            " nn_sentences:  ['this flower has four off white and purpley-pink petals, one sticking out of each side, and then a flat yellow white and pink pistil.\\n', 'this flower has wide and very smooth white petals and a yellow mouth.\\n']\n",
            "\n",
            "combined text:  117 original sentence:  the petals of the flower have various shades of green, and orange throughout.\n",
            " nn_sentences:  ['most of the flower is green and pointy, with many purple petals at the very top.\\n', 'this flower has many spiky green leaves along with a bunch of very thin purple petals.\\n']\n",
            "\n",
            "combined text:  118 original sentence:  this flower has small red petals and long red stamens.\n",
            " nn_sentences:  ['this is a yellow flower with a long green pedicel.\\n', 'this flower has a few amount of petals that are a very dark shade of purple.\\n']\n",
            "\n",
            "combined text:  119 original sentence:  the center of the flower is of various colors such as purple, maroon, white and yellow.\n",
            " nn_sentences:  ['this flower has a large number of very thin petals that are purple in color and arranged upright.\\n', 'this flower has red downturned petals and long pink filaments.\\n']\n",
            "\n",
            "combined text:  120 original sentence:  this flower has shell shaped orange and yellow highlighted petals.\n",
            " nn_sentences:  ['this flower has purple pestals on top and a geen pricky on bottom like a catus\\n', 'this flower has a large number of very thin petals that are purple in color and arranged upright.\\n']\n",
            "\n",
            "combined text:  121 original sentence:  this flower has different rows of pink petals with a layer of yellow petals at its center.\n",
            " nn_sentences:  ['the flower has many small pointy yellow petals with yellow anther in the center\\n', 'this flower has petals that are green and has purple stamen\\n']\n",
            "\n",
            "combined text:  122 original sentence:  this flower has a yellow color ovule with petals that are vanilla color.\n",
            " nn_sentences:  ['this flower is purple and red in color, and has petals that have veins.\\n', 'the plant in the photo is spikey and has a pink plant.\\n']\n",
            "\n",
            "combined text:  123 original sentence:  this flower is white and yellow in color, with petals that are oval shaped.\n",
            " nn_sentences:  ['the petals are yellow with anthers appearing as white.\\n', 'the flower has many small pointy yellow petals with yellow anther in the center\\n']\n",
            "\n",
            "combined text:  124 original sentence:  this flower has five smooth round petals in shades of white and pale purple, with a yellow center.\n",
            " nn_sentences:  ['this flower is yellow in color, and has petals that are spotted.\\n', 'this flower has petals that are pink and ruffled at the edges\\n']\n",
            "\n",
            "combined text:  125 original sentence:  this flower has lisgh pink petals with dark pink color at their edges and it has one pistil.\n",
            " nn_sentences:  ['the visible petals on the plant are thin and purple while the stem has spikes on it\\n', 'this flower has a large number of very thin petals that are purple in color and arranged upright.\\n']\n",
            "\n",
            "combined text:  126 original sentence:  the petals of this flower are yellow with a long stigma\n",
            " nn_sentences:  ['the petals are yellow with anthers appearing as white.\\n', 'this flower has a dense amount of small spiky purple petals.\\n']\n",
            "\n",
            "combined text:  127 original sentence:  a flower with purple petals and stamen and a yellow stigma.\n",
            " nn_sentences:  ['this flower has petals that are green and has purple stamen\\n', 'this flower has a large number of very thin petals that are purple in color and arranged upright.\\n']\n",
            "*** end of testing ***\n",
            "Running GAN-CLS took 1:55:35\n"
          ]
        }
      ]
    }
  ]
}